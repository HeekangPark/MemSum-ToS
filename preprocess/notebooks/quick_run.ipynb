{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import re\n",
    "from random import sample\n",
    "import logging\n",
    "from ftlangdetect import detect  \n",
    "# fasttext is the fastest & most accurate library for language detection, but requires manual downloading of a pre-trained model; \n",
    "# this library is a wrapper of fasttext and gets rid of the need of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_chrome_driver():\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 03:58:15,661 - [scrape_documents_per_service] - No document for (service: pragmaticblog)! https://edit.tosdr.org//services/1452/annotate\n"
     ]
    }
   ],
   "source": [
    "pragmaticblog_url = 'https://edit.tosdr.org//services/1452/annotate'\n",
    "pragmaticblog_html = scraper.get_html_source(pragmaticblog_url)\n",
    "scraper.scrape_documents_per_service('pragmaticblog', documents_html=pragmaticblog_html, url=pragmaticblog_url, out_file_name=\"./tosdr_pragmaticblog.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToSDRScraper():\n",
    "    def __init__(self, out_file=\"./tosdr.jsonl\", log_file='./test.log'):\n",
    "        self.driver = set_chrome_driver()\n",
    "        self.out_file = out_file\n",
    "        self.write_f = open(self.out_file, 'w')\n",
    "        self.base_url = 'https://edit.tosdr.org'\n",
    "        \n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - [%(funcName)s] - %(message)s',\n",
    "            handlers=[logging.FileHandler(log_file), logging.StreamHandler()]\n",
    "        )\n",
    "        self.logger = logging.getLogger('tosdr_logger')\n",
    "        \n",
    "        \n",
    "    def login(self, email='shparksue@gmail.com', password='2022-2NLPproject'):\n",
    "        \"\"\"Activates authenticated session\"\"\"\n",
    "        self.driver.get('https://edit.tosdr.org/users/sign_in')\n",
    "        \n",
    "        self.driver.find_element(By.ID, 'user_email').send_keys(email)\n",
    "        self.driver.find_element(By.ID, 'user_password').send_keys(password)\n",
    "        self.driver.find_element(By.XPATH, '//*[@id=\"new_user\"]/div[2]/input').click()\n",
    "    \n",
    "    \n",
    "    def get_html_source(self, url: str, timeout=0):\n",
    "        \"\"\"Get the HTML source to directly instantiate a new BeautifulSoup object (possibly for debugging purposes)\"\"\"\n",
    "        self.driver.get(url)\n",
    "        if timeout:\n",
    "            element = WebDriverWait(self.driver, timeout).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, 'table'))\n",
    "            )\n",
    "        return self.driver.page_source\n",
    "    \n",
    "    \n",
    "    def scrape_services(self):\n",
    "        \"\"\"Get urls for the annotated documents of each service and process each document\"\"\"\n",
    "        services_html = self.get_html_source(url='https://edit.tosdr.org/services', timeout=10)  # takes some time to load the full page\n",
    "        services_soup = BeautifulSoup(services_html, 'html.parser')\n",
    "        table = services_soup.select_one('table.table.table-striped')\n",
    "        all_services = table.find_all('tr', {'data-classification': ['A', 'B', 'C', 'D', 'E']})\n",
    "        print(\"Total number of services:\", len(all_services))\n",
    "        \n",
    "        for row in tqdm(all_services):\n",
    "            columns = row.find_all('td')\n",
    "            service = columns[1].text.strip()\n",
    "            url = self.base_url + columns[4].find('a', href=True)['href']\n",
    "            documents_html = self.get_html_source(url)\n",
    "            self.scrape_documents_per_service(service, url, documents_html)\n",
    "            \n",
    "            \n",
    "    def scrape_documents_per_service(self, service_name, url, documents_html, out_file_name=None):\n",
    "        \"\"\"Scrape each ToS document of each service and write the data into .jsonl format\"\"\"\n",
    "        if out_file_name:  # for DEBUG\n",
    "            self.out_file = out_file_name\n",
    "            self.write_f = open(self.out_file, 'w')\n",
    "            \n",
    "        documents_soup = BeautifulSoup(documents_html, 'html.parser')\n",
    "        documents = documents_soup.select('div.panel.panel-default')\n",
    "        if not documents:\n",
    "            self.logger.info(f\"No document for (service: {service_name})! {url}\")\n",
    "            return\n",
    "        \n",
    "        for document_elements in documents:\n",
    "            document_data = {'service': service_name, 'url': url}\n",
    "            document_data = self._parse_document(service_name, url, document_data, document_elements)\n",
    "            if not document_data:\n",
    "                continue\n",
    "            self.write_f.write(json.dumps(document_data) + '\\n')\n",
    "            \n",
    "    \n",
    "    def _parse_document(self, service_name, url, document_data, document_elements):\n",
    "        \"\"\"Parse each document return a dictionary of the structured information\"\"\"\n",
    "        title = document_elements.select_one('h3').text\n",
    "        original_text, summary = self._generate_document_data(document_elements)\n",
    "        if not original_text:\n",
    "            self.logger.info(f\"No content inside (service: {service_name}, document: {title})! {url}\")\n",
    "            return None\n",
    "        if not summary:\n",
    "            self.logger.info(f\"No annotation for (service: {service_name}, document: {title})! {url}\")\n",
    "            return None\n",
    "        if not self._detect_english(original_text):\n",
    "            self.logger.info(f\"Not English (service: {service_name}, document: {title})! {url}\")\n",
    "            return None\n",
    "        \n",
    "        return document_data | {'document_title': title,\n",
    "                'original_text_length': len(original_text),\n",
    "                'summary_length': len(summary),\n",
    "                'original_text': original_text, \n",
    "                'summary': summary}  # merge dictionary\n",
    "        \n",
    "        \n",
    "    def _generate_document_data(self, document_elements):\n",
    "        \"\"\"Iterate through the sections and divide the parsed sections of the documents into the original text and summary\"\"\"\n",
    "        # NOTE: the first or last sentence in the annotated section, and the sentences before and after it can contain incomplete phrases\n",
    "        full_doc = []\n",
    "        summary = []\n",
    "        ptr = document_elements.select_one('.panel-body.documentContent > p')\n",
    "        for section in ptr.next_siblings:  # iterate through the contents\n",
    "            if section.text.strip():  # tag content is not empty\n",
    "                sentences = self._parse_section(section.text)\n",
    "                if section.select('a'):  # is hyperlinked, i.e., annotated by users\n",
    "                    summary.extend(sentences)\n",
    "                full_doc.extend(sentences)\n",
    "        return full_doc, summary\n",
    "    \n",
    "    \n",
    "    def _parse_section(self, text):\n",
    "        \"\"\"Break down each section into sentences\"\"\"\n",
    "        sentences = []\n",
    "        for segments in re.split('<.+?>', text):\n",
    "            sentences.extend(segment.strip() for segment in segments.split('\\n') if segment.strip())\n",
    "        return sentences\n",
    "    \n",
    "    def _detect_english(self, original_text):\n",
    "        \"\"\"Returns True if the text is in English, otherwise False\"\"\"\n",
    "        # NOTE: At first try, it takes about 30s to download the pre-trained FastText model\n",
    "        result = set()\n",
    "        sentences = sample(original_text, 3)\n",
    "        for sentence in sentences:\n",
    "            result.add(detect(text=sentence)['lang'])\n",
    "        return True if 'en' in result else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 04:23:20,750 - [log] - ====== WebDriver manager ======\n",
      "2022-11-21 04:23:20,817 - [log] - Get LATEST chromedriver version for google-chrome 107.0.5304\n",
      "2022-11-21 04:23:21,168 - [log] - Driver [/Users/suepark/.wdm/drivers/chromedriver/mac_arm64/107.0.5304/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "scraper = ToSDRScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple documents & multiple annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_url = 'https://edit.tosdr.org//services/225/annotate'\n",
    "spotify_html = scraper.get_html_source(spotify_url)\n",
    "scraper.scrape_documents_per_service('Spotify', documents_html=spotify_html, url=spotify_url, out_file_name=\"./tosdr_spotify.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "instagram_url = 'https://edit.tosdr.org//services/219/annotate'\n",
    "instagram_html = scraper.get_html_source(instagram_url)\n",
    "scraper.scrape_documents_per_service('Instagram', documents_html=instagram_html, url=instagram_url, out_file_name=\"./tosdr_instagram.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 03:58:11,448 - [_parse_document] - Not English (service: SeenThis, document: Intellectual Property)! https://edit.tosdr.org//services/330/annotate\n"
     ]
    }
   ],
   "source": [
    "seenthis_url = 'https://edit.tosdr.org//services/330/annotate'\n",
    "seenthis_html = scraper.get_html_source(seenthis_url)\n",
    "scraper.scrape_documents_per_service('SeenThis', documents_html=seenthis_html, url=seenthis_url, out_file_name=\"./tosdr_seenthis.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 03:58:15,661 - [scrape_documents_per_service] - No document for (service: pragmaticblog)! https://edit.tosdr.org//services/1452/annotate\n"
     ]
    }
   ],
   "source": [
    "pragmaticblog_url = 'https://edit.tosdr.org//services/1452/annotate'\n",
    "pragmaticblog_html = scraper.get_html_source(pragmaticblog_url)\n",
    "scraper.scrape_documents_per_service('pragmaticblog', documents_html=pragmaticblog_html, url=pragmaticblog_url, out_file_name=\"./tosdr_pragmaticblog.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Document but 0 content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 03:58:18,597 - [_parse_document] - No content inside (service: gnome, document: Code of Conduct)! https://edit.tosdr.org//services/2781/annotate\n"
     ]
    }
   ],
   "source": [
    "gnome_url = 'https://edit.tosdr.org//services/2781/annotate'\n",
    "gnome_html = scraper.get_html_source(gnome_url)\n",
    "scraper.scrape_documents_per_service('gnome', documents_html=gnome_html, url=gnome_url, out_file_name=\"./tosdr_gnome.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document with 0 annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 03:58:22,607 - [_parse_document] - No annotation for (service: musicbrainz, document: Code of Conduct)! https://edit.tosdr.org//services/736/annotate\n"
     ]
    }
   ],
   "source": [
    "musicbrainz_url = 'https://edit.tosdr.org//services/736/annotate'\n",
    "musicbrainz_html = scraper.get_html_source(musicbrainz_url)\n",
    "scraper.scrape_documents_per_service('musicbrainz', documents_html=musicbrainz_html, url=musicbrainz_url, out_file_name=\"./tosdr_musicbrainz.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of services: 1255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1255 [00:08<58:17,  2.79s/it]2022-11-21 04:23:58,102 - [_parse_document] - No annotation for (service: Flickr, document: Flickr Privacy Policy)! https://edit.tosdr.org/services/186/annotate\n",
      "2022-11-21 04:23:58,104 - [_parse_document] - No annotation for (service: Flickr, document: Community Guidelines)! https://edit.tosdr.org/services/186/annotate\n",
      "2022-11-21 04:23:58,110 - [_parse_document] - No annotation for (service: Flickr, document: Data Processing Policy - Date Uncertain, Sometime during or after 2017)! https://edit.tosdr.org/services/186/annotate\n",
      "2022-11-21 04:23:58,131 - [_parse_document] - No annotation for (service: Flickr, document: Community Guidelines - April 30th 2020)! https://edit.tosdr.org/services/186/annotate\n",
      "  0%|          | 5/1255 [00:13<56:24,  2.71s/it]  2022-11-21 04:24:03,487 - [_parse_document] - No annotation for (service: Goguardian, document: Product Terms)! https://edit.tosdr.org/services/1625/annotate\n",
      "2022-11-21 04:24:03,491 - [_parse_document] - No annotation for (service: Goguardian, document: Product Privacy Policy)! https://edit.tosdr.org/services/1625/annotate\n",
      "  0%|          | 6/1255 [00:16<1:00:07,  2.89s/it]2022-11-21 04:24:05,986 - [_parse_document] - No content inside (service: Blizzard, document: Merged into document 497 by user 6)! https://edit.tosdr.org/services/539/annotate\n",
      "  1%|          | 8/1255 [00:22<58:17,  2.80s/it]  2022-11-21 04:24:11,594 - [_parse_document] - No annotation for (service: Foursquare, document: Cookie policy)! https://edit.tosdr.org/services/251/annotate\n",
      "  1%|          | 14/1255 [00:39<57:43,  2.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scraper\u001b[39m.\u001b[39;49mscrape_services()\n",
      "Cell \u001b[0;32mIn [111], line 49\u001b[0m, in \u001b[0;36mToSDRScraper.scrape_services\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m service \u001b[39m=\u001b[39m columns[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     48\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_url \u001b[39m+\u001b[39m columns[\u001b[39m4\u001b[39m]\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m, href\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 49\u001b[0m documents_html \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_html_source(url)\n\u001b[1;32m     50\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscrape_documents_per_service(service, url, documents_html)\n",
      "Cell \u001b[0;32mIn [111], line 29\u001b[0m, in \u001b[0;36mToSDRScraper.get_html_source\u001b[0;34m(self, url, timeout)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_html_source\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m     28\u001b[0m     \u001b[39m\"\"\"Get the HTML source to directly instantiate a new BeautifulSoup object (possibly for debugging purposes)\"\"\"\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdriver\u001b[39m.\u001b[39;49mget(url)\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m timeout:\n\u001b[1;32m     31\u001b[0m         element \u001b[39m=\u001b[39m WebDriverWait(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdriver, timeout)\u001b[39m.\u001b[39muntil(\n\u001b[1;32m     32\u001b[0m             EC\u001b[39m.\u001b[39mpresence_of_element_located((By\u001b[39m.\u001b[39mTAG_NAME, \u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     33\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:455\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m    Loads a web page in the current browser session.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py:442\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    440\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m--> 442\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    443\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    444\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:294\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    292\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    293\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 294\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py:316\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    313\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 316\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    317\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1378\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tos-crawler/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scraper.scrape_services()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
