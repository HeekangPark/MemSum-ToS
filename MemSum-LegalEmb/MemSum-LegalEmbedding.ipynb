{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MemSum with Legal Domain Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2PqJpw3FcTS"
   },
   "source": [
    "In this notebook we will train MemSum with different word embeddings pretrained on a legal domain.\n",
    "\n",
    "The following models where trained\n",
    "\n",
    "1. train ToS;DR dataset with legal embeddings\n",
    "2. train GovReport dataset with legal embeddings (utilize MemSum's GovReport checkpoint) \n",
    "3. fine-tune ToS;DR on 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fw1sOX0AEUzV",
    "outputId": "541e5337-60ad-4d4b-c222-703536d3c2c0"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "Ky4m3pQNSAVB",
    "outputId": "f1825438-8223-4050-81e6-fa8ae600448b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu116'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Download pretrained word embedding & model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NW2PUEZJK-rT",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "legal domain word2vec models from https://osf.io/qvg8s/\n",
    "\n",
    "modified to fit MemSum(vector size set to 200 & \\<eod>, \\<pad>, \\<unk> tokens added)\n",
    "\n",
    "Download pickle film & model checkpoints from google drive(https://drive.google.com/drive/folders/1za083ah4oPjX14uYH8rwfbtoE-wiFJcP?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ojiaw0SSK7vj",
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 13 23:22:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:05:00.0 Off |                  N/A |\n",
      "| 44%   50C    P8    18W / 320W |      3MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX    On   | 00000000:09:00.0 Off |                  N/A |\n",
      "| 41%   42C    P8     8W / 280W |   3444MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    1   N/A  N/A     10758      C   ...a3/envs/memsum/bin/python     3441MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W50ktMCZNyz9",
    "tags": []
   },
   "source": [
    "### 1. ToS;DR + Legal Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wPnWLxpkMYLy",
    "outputId": "5e074851-4bcb-42a0-b13a-a8e847f7a521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611it [00:00, 10911.57it/s]\n",
      "202it [00:00, 12187.13it/s]\n",
      "model restored!\n",
      "optimizer restored!\n",
      "[current_epoch: 2] \n",
      "current_batch restored!\n",
      "0it [00:00, ?it/s]/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:228: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  remaining_mask_np = np.ones_like( doc_mask_np ).astype( np.bool ) | doc_mask_np\n",
      "/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:229: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  extraction_mask_np = np.zeros_like( doc_mask_np ).astype( np.bool ) | doc_mask_np\n",
      "99it [00:50,  2.13it/s][current_batch: 01200] loss: 4.098, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:309: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  remaining_mask_np = np.ones_like( doc_mask ).astype( np.bool ) | doc_mask\n",
      "/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:310: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  extraction_mask_np = np.zeros_like( doc_mask ).astype( np.bool ) | doc_mask\n",
      "val: 0.4021, 0.2479, 0.3883\n",
      "199it [01:51,  1.97it/s][current_batch: 01300] loss: 3.976, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4020, 0.2468, 0.3884\n",
      "299it [02:52,  1.87it/s][current_batch: 01400] loss: 3.990, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4037, 0.2496, 0.3904\n",
      "399it [03:53,  1.99it/s][current_batch: 01500] loss: 3.958, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4054, 0.2530, 0.3921\n",
      "401it [04:06,  3.09s/it]Starting validation ...\n",
      "val: 0.4055, 0.2530, 0.3922\n",
      "402it [04:19,  1.55it/s]\n",
      "97it [00:46,  1.87it/s][current_batch: 01600] loss: 3.806, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4076, 0.2566, 0.3942\n",
      "197it [01:47,  1.41it/s][current_batch: 01700] loss: 3.972, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4077, 0.2549, 0.3944\n",
      "297it [02:48,  2.10it/s][current_batch: 01800] loss: 3.870, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4076, 0.2552, 0.3944\n",
      "397it [03:49,  1.98it/s][current_batch: 01900] loss: 3.924, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4090, 0.2567, 0.3959\n",
      "401it [04:04,  2.00s/it]Starting validation ...\n",
      "val: 0.4088, 0.2568, 0.3956\n",
      "402it [04:17,  1.56it/s]\n",
      "95it [00:47,  1.80it/s][current_batch: 02000] loss: 3.675, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4108, 0.2608, 0.3984\n",
      "195it [01:49,  1.56it/s][current_batch: 02100] loss: 3.839, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4105, 0.2610, 0.3982\n",
      "295it [02:51,  1.79it/s][current_batch: 02200] loss: 3.896, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4138, 0.2659, 0.4014\n",
      "395it [03:54,  1.90it/s][current_batch: 02300] loss: 3.861, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4134, 0.2652, 0.4012\n",
      "401it [04:10,  1.33s/it]Starting validation ...\n",
      "val: 0.4134, 0.2653, 0.4011\n",
      "402it [04:23,  1.53it/s]\n",
      "93it [00:43,  1.83it/s][current_batch: 02400] loss: 3.420, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4154, 0.2679, 0.4030\n",
      "193it [01:43,  2.34it/s][current_batch: 02500] loss: 3.769, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4140, 0.2677, 0.4015\n",
      "293it [02:45,  2.03it/s][current_batch: 02600] loss: 3.841, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4149, 0.2692, 0.4025\n",
      "393it [03:48,  2.15it/s][current_batch: 02700] loss: 3.794, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4155, 0.2706, 0.4032\n",
      "401it [04:04,  1.35it/s]Starting validation ...\n",
      "val: 0.4154, 0.2705, 0.4031\n",
      "402it [04:17,  1.56it/s]\n",
      "91it [00:45,  2.07it/s][current_batch: 02800] loss: 3.371, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4138, 0.2695, 0.4014\n",
      "191it [01:44,  2.22it/s][current_batch: 02900] loss: 3.662, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4149, 0.2720, 0.4025\n",
      "291it [02:47,  1.89it/s][current_batch: 03000] loss: 3.668, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4146, 0.2712, 0.4021\n",
      "391it [03:47,  2.39it/s][current_batch: 03100] loss: 3.720, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4157, 0.2720, 0.4031\n",
      "401it [04:04,  1.54it/s]Starting validation ...\n",
      "val: 0.4157, 0.2720, 0.4030\n",
      "402it [04:16,  1.56it/s]\n",
      "89it [00:45,  1.37it/s][current_batch: 03200] loss: 3.218, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4138, 0.2689, 0.4012\n",
      "189it [01:46,  1.82it/s][current_batch: 03300] loss: 3.641, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4138, 0.2681, 0.4009\n",
      "289it [02:45,  2.36it/s][current_batch: 03400] loss: 3.563, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4160, 0.2712, 0.4031\n",
      "389it [03:44,  2.14it/s][current_batch: 03500] loss: 3.612, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4161, 0.2727, 0.4036\n",
      "401it [04:01,  1.64it/s]Starting validation ...\n",
      "val: 0.4161, 0.2727, 0.4036\n",
      "402it [04:14,  1.58it/s]\n",
      "87it [00:42,  2.55it/s][current_batch: 03600] loss: 3.115, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4152, 0.2727, 0.4030\n",
      "104it [01:04,  1.93it/s]"
     ]
    }
   ],
   "source": [
    "!cd src/MemSum_Full; python train.py -training_corpus_file_name ../../tosdata/train_labelled.jsonl -validation_corpus_file_name ../../tosdata/validation.jsonl -model_folder ../../model/just_tos/run1/ -log_folder ../../log/just_tos/run1/ -vocabulary_file_name ../../sigmalaw/SigmaLaw_Vocab_200dim.pkl -pretrained_unigram_embeddings_file_name ../../sigmalaw/SigmaLaw_Embeddings_200dim.pkl -max_seq_len 50 -max_doc_len 300 -num_of_epochs 15 -save_every 100 -n_device 1 -batch_size_per_device 4 -max_extracted_sentences_per_document 13 -moving_average_decay 0.999 -p_stop_thres 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2. GovReport + Legal Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17517it [00:04, 4338.48it/s]\n",
      "974it [00:00, 5103.27it/s]\n",
      "model restored!\n",
      "optimizer restored!\n",
      "[current_epoch: 14] \n",
      "current_batch restored!\n"
     ]
    }
   ],
   "source": [
    "!cd src/MemSum_Full; python train.py -training_corpus_file_name ../../data/train_GOVREPORT.jsonl -validation_corpus_file_name ../../data/val_GOVREPORT.jsonl -model_folder ../../model/gov/run1 -log_folder ../../log/gov/run1 -vocabulary_file_name ../../sigmalaw/SigmaLaw_Vocab_200dim.pkl -pretrained_unigram_embeddings_file_name ../../sigmalaw/SigmaLaw_Embeddings_200dim.pkl -restore_old_checkpoint True -max_seq_len 100 -max_doc_len 500 -num_of_epochs 14 -save_every 1000 -n_device 1 -batch_size_per_device 4 -max_extracted_sentences_per_document 22 -moving_average_decay 0.999 -p_stop_thres 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3. GovReport + Legal Embeddings + ToS;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use model_batch_63000 from 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611it [00:00, 7491.95it/s]\n",
      "202it [00:00, 10440.15it/s]\n",
      "model restored!\n",
      "optimizer restored!\n",
      "[current_epoch: 0] \n",
      "current_batch restored!\n",
      "0it [00:00, ?it/s]/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:228: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  remaining_mask_np = np.ones_like( doc_mask_np ).astype( np.bool ) | doc_mask_np\n",
      "/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:229: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  extraction_mask_np = np.zeros_like( doc_mask_np ).astype( np.bool ) | doc_mask_np\n",
      "99it [00:56,  1.94it/s][current_batch: 00100] loss: 4.040, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:309: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  remaining_mask_np = np.ones_like( doc_mask ).astype( np.bool ) | doc_mask\n",
      "/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py:310: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  extraction_mask_np = np.zeros_like( doc_mask ).astype( np.bool ) | doc_mask\n",
      "val: 0.3786, 0.2213, 0.3645\n",
      "199it [02:04,  1.29it/s][current_batch: 00200] loss: 4.012, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3846, 0.2314, 0.3708\n",
      "299it [03:16,  1.91it/s][current_batch: 00300] loss: 4.054, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3852, 0.2330, 0.3714\n",
      "399it [04:24,  2.37it/s][current_batch: 00400] loss: 3.962, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3839, 0.2349, 0.3707\n",
      "401it [04:41,  3.74s/it]Starting validation ...\n",
      "val: 0.3844, 0.2358, 0.3711\n",
      "402it [04:58,  1.35it/s]\n",
      "97it [00:45,  2.05it/s][current_batch: 00500] loss: 3.750, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3870, 0.2407, 0.3741\n",
      "197it [01:51,  2.18it/s][current_batch: 00600] loss: 3.802, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3909, 0.2456, 0.3782\n",
      "297it [02:57,  1.45it/s][current_batch: 00700] loss: 3.965, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3946, 0.2506, 0.3818\n",
      "397it [04:02,  1.95it/s][current_batch: 00800] loss: 3.820, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3984, 0.2553, 0.3855\n",
      "401it [04:20,  2.21s/it]Starting validation ...\n",
      "val: 0.3983, 0.2555, 0.3855\n",
      "402it [04:35,  1.46it/s]\n",
      "95it [00:47,  2.12it/s][current_batch: 00900] loss: 3.618, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4002, 0.2563, 0.3874\n",
      "195it [01:51,  2.49it/s][current_batch: 01000] loss: 3.772, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4018, 0.2593, 0.3893\n",
      "295it [02:53,  1.90it/s][current_batch: 01100] loss: 3.633, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4030, 0.2603, 0.3902\n",
      "395it [03:56,  1.72it/s][current_batch: 01200] loss: 3.698, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4029, 0.2603, 0.3902\n",
      "401it [04:12,  1.13s/it]Starting validation ...\n",
      "val: 0.4031, 0.2612, 0.3904\n",
      "402it [04:27,  1.50it/s]\n",
      "93it [00:47,  2.00it/s][current_batch: 01300] loss: 3.328, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4026, 0.2600, 0.3897\n",
      "193it [01:48,  2.18it/s][current_batch: 01400] loss: 3.584, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4058, 0.2653, 0.3932\n",
      "293it [02:50,  2.23it/s][current_batch: 01500] loss: 3.550, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4048, 0.2650, 0.3924\n",
      "393it [03:50,  2.35it/s][current_batch: 01600] loss: 3.649, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4045, 0.2655, 0.3920\n",
      "401it [04:07,  1.28it/s]Starting validation ...\n",
      "val: 0.4049, 0.2660, 0.3925\n",
      "402it [04:21,  1.54it/s]\n",
      "91it [00:46,  2.30it/s][current_batch: 01700] loss: 3.093, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.4006, 0.2632, 0.3887\n",
      "191it [01:46,  2.19it/s][current_batch: 01800] loss: 3.451, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3996, 0.2622, 0.3877\n",
      "291it [02:51,  2.15it/s][current_batch: 01900] loss: 3.402, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3994, 0.2616, 0.3879\n",
      "391it [03:54,  1.69it/s][current_batch: 02000] loss: 3.489, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3981, 0.2610, 0.3866\n",
      "401it [04:11,  1.63it/s]Starting validation ...\n",
      "val: 0.3987, 0.2616, 0.3872\n",
      "402it [04:24,  1.52it/s]\n",
      "89it [00:44,  2.06it/s][current_batch: 02100] loss: 2.922, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3999, 0.2630, 0.3881\n",
      "189it [01:45,  2.32it/s][current_batch: 02200] loss: 3.267, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3989, 0.2629, 0.3871\n",
      "289it [02:46,  2.01it/s][current_batch: 02300] loss: 3.149, learning rate: 0.000100\n",
      "Starting validation ...\n",
      "val: 0.3978, 0.2637, 0.3865\n",
      "315it [03:12,  2.04it/s]^C\n",
      "315it [03:12,  1.64it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py\", line 362, in <module>\n",
      "    loss = train_iteration(batch)\n",
      "  File \"/home/seungminahan/workspace/memsum/MemSum/src/MemSum_Full/train.py\", line 291, in train_iteration\n",
      "    loss.backward()    \n",
      "  File \"/home/seungminahan/miniconda3/envs/memsum/lib/python3.10/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/seungminahan/miniconda3/envs/memsum/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd src/MemSum_Full; python train.py -training_corpus_file_name ../../tosdata/train_labelled.jsonl -validation_corpus_file_name ../../tosdata/validation.jsonl -model_folder ../../model/gov_tos/run1/ -log_folder ../../log/gov_tos/run1/ -vocabulary_file_name ../../sigmalaw/SigmaLaw_Vocab_200dim.pkl -pretrained_unigram_embeddings_file_name ../../sigmalaw/SigmaLaw_Embeddings_200dim.pkl -max_seq_len 50 -max_doc_len 300 -num_of_epochs 10 -save_every 100 -n_device 1 -batch_size_per_device 4 -max_extracted_sentences_per_document 13 -moving_average_decay 0.999 -p_stop_thres 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47uWDs5nSZqA",
    "tags": []
   },
   "source": [
    "# Testing trained model on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5ziC45_cSfu9"
   },
   "outputs": [],
   "source": [
    "from summarizers import MemSum\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "bcAh047kSjLj"
   },
   "outputs": [],
   "source": [
    "rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)\n",
    "\n",
    "tosdr_legal = MemSum(  \"model/just_tos/run2/model_batch_4300.pt\", \n",
    "                  \"sigmalaw/SigmaLaw_Vocab_200dim.pkl\", \n",
    "                  gpu = 0 ,  max_doc_len = 300  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_legal = MemSum(  \"model/gov/run1/model_batch_63000.pt\", \n",
    "                  \"sigmalaw/SigmaLaw_Vocab_200dim.pkl\", \n",
    "                  gpu = 0 ,  max_doc_len = 500  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosdr_gov_legal = MemSum(  \"model/gov_tos/run1/model_batch_1500.pt\", \n",
    "                  \"sigmalaw/SigmaLaw_Vocab_200dim.pkl\", \n",
    "                  gpu = 0 ,  max_doc_len = 300  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "FJODv4CcUVwb"
   },
   "outputs": [],
   "source": [
    "test_data = [ json.loads(line) for line in open(\"tosdata/test.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gov = [ json.loads(line) for line in open(\"data/test_GOVREPORT.jsonl\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "c6f3Up_1UiWz"
   },
   "outputs": [],
   "source": [
    "def evaluate( model, corpus, p_stop, max_extracted_sentences, rouge_cal ):\n",
    "    scores = []\n",
    "    for data in tqdm(corpus):\n",
    "        gold_summary = data[\"summary\"]\n",
    "        extracted_summary = model.extract( [data[\"text\"]], p_stop_thres = p_stop, max_extracted_sentences_per_document = max_extracted_sentences )[0]\n",
    "        \n",
    "        score = rouge_cal.score( \"\\n\".join( gold_summary ), \"\\n\".join(extracted_summary)  )\n",
    "        scores.append( [score[\"rouge1\"].fmeasure, score[\"rouge2\"].fmeasure, score[\"rougeLsum\"].fmeasure ] )\n",
    "    \n",
    "    return np.asarray(scores).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaUWUcW0Ul9t",
    "outputId": "1ab86c06-d9d0-4753-b18e-49e5f343a077"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201/201 [00:27<00:00,  7.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.4141877 , 0.27054185, 0.40019928])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate( tosdr_legal, test_data, 0.6, 13, rouge_cal )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 973/973 [06:20<00:00,  2.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.593514  , 0.28232396, 0.56586892])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate( gov_legal, test_gov, 0.6, 22, rouge_cal )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 201/201 [00:26<00:00,  7.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.40880941, 0.26623319, 0.39521398])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate( tosdr_gov_legal, test_data, 0.6, 13, rouge_cal )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1ra5Yp2Vj2O"
   },
   "source": [
    "To cite MemSum, please use the following bibtex:\n",
    "\n",
    "```\n",
    "@inproceedings{gu-etal-2022-memsum,\n",
    "    title = \"{M}em{S}um: Extractive Summarization of Long Documents Using Multi-Step Episodic {M}arkov Decision Processes\",\n",
    "    author = \"Gu, Nianlong  and\n",
    "      Ash, Elliott  and\n",
    "      Hahnloser, Richard\",\n",
    "    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n",
    "    month = may,\n",
    "    year = \"2022\",\n",
    "    address = \"Dublin, Ireland\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2022.acl-long.450\",\n",
    "    doi = \"10.18653/v1/2022.acl-long.450\",\n",
    "    pages = \"6507--6522\",\n",
    "    abstract = \"We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum{'}s awareness of extraction history.\",\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
