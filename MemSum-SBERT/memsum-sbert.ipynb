{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6dc880d-e3e6-458e-8a10-9a07f18e0dc6",
   "metadata": {},
   "source": [
    "# Memsum-SBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f5a33-fc9b-4b8b-a83f-085454cbf533",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2683d0ad-7b66-49cd-8738-640b6aaf2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3d513e3-9442-40bb-8a81-20408d323538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from hk_utils import print\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from src.MemSum_Full.model import AddMask, PositionalEncoding, MultiHeadAttention, FeedForward, TransformerEncoderLayer, TransformerDecoderLayer, MultiHeadPoolingLayer\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1fdcd96-8789-4fc9-9d4e-ca40e9944de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu analyzer\n",
    "import gc\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "    if size_bytes == 0:\n",
    "        return \"0B\"\n",
    "    size_name = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "    i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "    p = math.pow(1024, i)\n",
    "    s = round(size_bytes / p, 2)\n",
    "    return \"%s%s\" % (s, size_name[i])\n",
    "\n",
    "def analyzeGPU():\n",
    "    tensor_num = 0\n",
    "    tensor_size = 0\n",
    "    param_num = 0\n",
    "    param_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda:\n",
    "                if type(obj) is torch.nn.parameter.Parameter:\n",
    "                    param_num += 1\n",
    "                    param_size += obj.element_size() * obj.nelement()\n",
    "                else:\n",
    "                    #tensors.append(obj)\n",
    "                    tensor_num += 1\n",
    "                    tensor_size += obj.element_size() * obj.nelement()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return (tensor_num, convert_size(tensor_size)), (param_num, convert_size(param_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064e4925-85fa-4ea5-93de-582ca33d1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def print_time(time_in_sec):\n",
    "    if time_in_sec <= 60:\n",
    "        seconds = time_in_sec\n",
    "        return f\"{seconds:.3f}s\"\n",
    "    elif 60 < time_in_sec <= 60 * 60:\n",
    "        minutes = int(time_in_sec // 60)\n",
    "        seconds = time_in_sec - (minutes * 60)\n",
    "        return f\"{minutes:02}m {seconds:.3f}s\"\n",
    "    elif 60 * 60 < time_in_sec <= 60 * 60 * 24:\n",
    "        hours = int(time_in_sec // (60 * 60))\n",
    "        minutes = int(( time_in_sec - hours * (60 * 60) ) // 60)\n",
    "        seconds = time_in_sec - hours * (60 * 60) - minutes * 60\n",
    "        return f\"{hours:02}h {minutes:02}m {seconds:.3f}s\"\n",
    "    else:\n",
    "        days = int(time_in_sec % (60 * 60 * 24))\n",
    "        hours = int(( time_in_sec - days * (60 * 60 * 24) ) // (60 * 60))\n",
    "        minutes = int(( time_in_sec - days * (60 * 60 * 24) - hours * (60 * 60) ) // 60)\n",
    "        seconds = time_in_sec - days * (60 * 60 * 24) - hours * (60 * 60) - minutes * 60\n",
    "        return f\"{days}d {hours:02}h {minutes:02}m {seconds:.3f}s\"\n",
    "    \n",
    "def get_ngram(sentence, n, delim=\"_\"):\n",
    "    words = sentence.lower().split()\n",
    "    \n",
    "    ngram_set = set()\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram_set.add(delim.join(words[i:i + n]))\n",
    "        \n",
    "    return ngram_set\n",
    "\n",
    "def update_moving_average( m_ema, m, decay=0.999 ):\n",
    "    with torch.no_grad():\n",
    "        param_dict_m_ema = m_ema.parameters() \n",
    "        param_dict_m = m.parameters() \n",
    "        for param_m_ema, param_m in zip( param_dict_m_ema, param_dict_m ):\n",
    "            param_m_ema.copy_( decay * param_m_ema + (1 - decay) * param_m )\n",
    "            \n",
    "def save_model(batch, scores, model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    data = {\n",
    "        \"sbert\": sbert_model_name,\n",
    "        \"current_batch\": batch,\n",
    "        \"global_context_encoder\": global_context_encoder_ema.state_dict(),\n",
    "        \"extraction_context_decoder\": extraction_context_decoder_ema.state_dict(),\n",
    "        \"extractor\": extractor_ema.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"scores\": scores\n",
    "    }\n",
    "    \n",
    "    filename = f\"{batch:06}.{scores['rouge1']:.3f}-{scores['rouge2']:.3f}-{scores['rougeL']:.3f}.pt\"\n",
    "    model_path = os.path.join(model_dir, filename)\n",
    "    \n",
    "    torch.save(data, model_path)\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "def load_model(model_path, device):\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    global_context_encoder.load_state_dict(ckpt[\"global_context_encoder\"])\n",
    "    extraction_context_decoder.load_state_dict(ckpt[\"extraction_context_decoder\"])\n",
    "    extractor.load_state_dict(ckpt[\"extractor\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b389d347-eb41-4715-9d96-406a8b282174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "class LocalSentenceEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        input_type=\"packed\",\n",
    "        max_sentence_num=500\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.input_type = input_type\n",
    "        self.max_sentence_num = max_sentence_num\n",
    "        \n",
    "    @property\n",
    "    def embed_dim(self):\n",
    "        return self.model.config.hidden_size\n",
    "    \n",
    "    def _forward_packed_input(self, input_ids, attention_mask, sentence_nums):\n",
    "        # input_ids : tensor[sum(sentence_nums), max_seq_len]\n",
    "        # attention_mask : tensor[sum(sentence_nums), max_seq_len]\n",
    "        # sentence_nums : list[batch_size]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            token_embeddings = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            ).last_hidden_state\n",
    "            \n",
    "            # mean pooling\n",
    "            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            output = torch.sum(token_embeddings * attention_mask_expanded, 1) / torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "            # normalization\n",
    "            sentence_embeddings = F.normalize(output, p=2, dim=1)\n",
    "            \n",
    "            # reshape : tensor[sum(sentence_nums), emb_dim] -> tensor[batch_size, max_sent_num, emb_dim]\n",
    "            sentence_embeddings_by_doc = []\n",
    "            idx_cumsum = 0\n",
    "            for sentence_num in sentence_nums:\n",
    "                padded_sentence_embeddings = torch.zeros([self.max_sentence_num, sentence_embeddings.shape[-1]]).to(input_ids.device)\n",
    "                padded_sentence_embeddings[:sentence_num, :] = sentence_embeddings[idx_cumsum:idx_cumsum + sentence_num]\n",
    "                sentence_embeddings_by_doc.append(padded_sentence_embeddings)\n",
    "                \n",
    "                idx_cumsum += sentence_num\n",
    "                \n",
    "            sentence_embeddings = torch.stack(sentence_embeddings_by_doc, dim=0)\n",
    "            \n",
    "            return sentence_embeddings  # tensor[batch_size, max_sent_num, emb_dim]\n",
    "        \n",
    "    def _forward_padded_input(self, input_ids, attention_mask):\n",
    "        # input_ids : tensor[batch_size * max_sent_num, max_seq_len]\n",
    "        # attention_mask : tensor[batch_size * max_sent_num, max_seq_len]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = input_ids.shape[0] // self.max_sentence_num\n",
    "            \n",
    "            token_embeddings = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            ).last_hidden_state\n",
    "            \n",
    "            # mean pooling\n",
    "            attention_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            output = torch.sum(token_embeddings * attention_mask_expanded, 1) / torch.clamp(attention_mask_expanded.sum(1), min=1e-9)\n",
    "            \n",
    "            # normalization\n",
    "            sentence_embeddings = F.normalize(output, p=2, dim=1)\n",
    "            \n",
    "            # reshape : tensor[batch_size * max_sent_num, emb_dim] -> tensor[batch_size, max_sent_num, emb_dim]\n",
    "            sentence_embeddings_by_doc = []\n",
    "            for i in range(batch_size):\n",
    "                sentence_embeddings_by_doc.append(sentence_embeddings[i * self.max_sentence_num:(i + 1) * self.max_sentence_num, :])\n",
    "            sentence_embeddings = torch.stack(sentence_embeddings_by_doc, dim=0)\n",
    "            \n",
    "            return sentence_embeddings  # tensor[batch_size, max_sent_num, emb_dim]\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, sentence_nums=None):\n",
    "        if self.input_type == \"packed\":\n",
    "            assert type(sentence_nums) is list, \"Invalid input:sentence_nums\"\n",
    "            return self._forward_packed_input(input_ids, attention_mask, sentence_nums)\n",
    "        elif self.input_type == \"padded\":\n",
    "            return self._forward_padded_input(input_ids, attention_mask)\n",
    "\n",
    "class GlobalContextEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim,  num_heads, hidden_dim, num_dec_layers ):\n",
    "        super().__init__()\n",
    "        # self.pos_encode = PositionalEncoding( embed_dim)\n",
    "        # self.layer_list = nn.ModuleList( [  TransformerEncoderLayer( embed_dim, num_heads, hidden_dim ) for _ in range(num_dec_layers) ] )\n",
    "        self.rnn = nn.LSTM(  embed_dim, embed_dim, 2, batch_first = True, bidirectional = True)\n",
    "        self.norm_out = nn.LayerNorm( 2*embed_dim )\n",
    "        self.ln_out = nn.Linear( 2*embed_dim, embed_dim )\n",
    "\n",
    "    def forward(self, sen_embed, doc_mask, dropout_rate = 0.):\n",
    "        net, _ = self.rnn( sen_embed )\n",
    "        net = self.ln_out(F.relu( self.norm_out(net) ) )\n",
    "        return net\n",
    "\n",
    "class ExtractionContextDecoder( nn.Module ):\n",
    "    def __init__( self, embed_dim,  num_heads, hidden_dim, num_dec_layers ):\n",
    "        super().__init__()\n",
    "        self.layer_list = nn.ModuleList( [  TransformerDecoderLayer( embed_dim, num_heads, hidden_dim ) for _ in range(num_dec_layers) ] )\n",
    "    ## remaining_mask: set all unextracted sen indices as True\n",
    "    ## extraction_mask: set all extracted sen indices as True\n",
    "    def forward( self, sen_embed, remaining_mask, extraction_mask, dropout_rate = 0. ):\n",
    "        net = sen_embed\n",
    "        for layer in self.layer_list:\n",
    "            #  encoder_output, x,  src_mask, trg_mask , dropout_rate = 0.\n",
    "            net = layer( sen_embed, net, remaining_mask, extraction_mask, dropout_rate )\n",
    "        return net\n",
    "\n",
    "class Extractor( nn.Module ):\n",
    "    def __init__( self, embed_dim, num_heads ):\n",
    "        super().__init__()\n",
    "        self.norm_input = nn.LayerNorm( 3*embed_dim  )\n",
    "        \n",
    "        self.ln_hidden1 = nn.Linear(  3*embed_dim, 2*embed_dim  )\n",
    "        self.norm_hidden1 = nn.LayerNorm( 2*embed_dim  )\n",
    "        \n",
    "        self.ln_hidden2 = nn.Linear(  2*embed_dim, embed_dim  )\n",
    "        self.norm_hidden2 = nn.LayerNorm( embed_dim  )\n",
    "\n",
    "        self.ln_out = nn.Linear(  embed_dim, 1 )\n",
    "\n",
    "        self.mh_pool = MultiHeadPoolingLayer( embed_dim, num_heads )\n",
    "        self.norm_pool = nn.LayerNorm( embed_dim  )\n",
    "        self.ln_stop = nn.Linear(  embed_dim, 1 )\n",
    "\n",
    "        self.mh_pool_2 = MultiHeadPoolingLayer( embed_dim, num_heads )\n",
    "        self.norm_pool_2 = nn.LayerNorm( embed_dim  )\n",
    "        self.ln_baseline = nn.Linear(  embed_dim, 1 )\n",
    "\n",
    "    def forward( self, sen_embed, relevance_embed, redundancy_embed , extraction_mask, dropout_rate = 0. ):\n",
    "        if redundancy_embed is None:\n",
    "            redundancy_embed = torch.zeros_like( sen_embed )\n",
    "        net = self.norm_input( F.dropout( torch.cat( [ sen_embed, relevance_embed, redundancy_embed ], dim = 2 ) , p = dropout_rate  )  ) \n",
    "        net = F.relu( self.norm_hidden1( F.dropout( self.ln_hidden1( net ) , p = dropout_rate  )   ))\n",
    "        hidden_net = F.relu( self.norm_hidden2( F.dropout( self.ln_hidden2( net)  , p = dropout_rate  )  ))\n",
    "        \n",
    "        p = self.ln_out( hidden_net ).sigmoid().squeeze(2)\n",
    "\n",
    "        net = F.relu( self.norm_pool(  F.dropout( self.mh_pool( hidden_net, extraction_mask) , p = dropout_rate  )  ))\n",
    "        p_stop = self.ln_stop( net ).sigmoid().squeeze(1)\n",
    "\n",
    "        net = F.relu( self.norm_pool_2(  F.dropout( self.mh_pool_2( hidden_net, extraction_mask ) , p = dropout_rate  )  ))\n",
    "        baseline = self.ln_baseline(net)\n",
    "\n",
    "        return p, p_stop, baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a61f12-80a5-424a-a49a-bedf8ba53a50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## GovReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7308801d-220c-45ea-8558-c3694e305ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instances\n",
    "max_sentence_num = 500\n",
    "max_sequence_len = 100\n",
    "\n",
    "sbert_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "local_sentence_encoder = LocalSentenceEncoder(\n",
    "    model_name=sbert_model_name,\n",
    "    input_type=\"packed\",\n",
    "    max_sentence_num=max_sentence_num\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(sbert_model_name)\n",
    "embed_dim = local_sentence_encoder.embed_dim\n",
    "\n",
    "global_context_encoder = GlobalContextEncoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=8,\n",
    "    hidden_dim=1024,\n",
    "    num_dec_layers=3\n",
    ").to(device)\n",
    "\n",
    "extraction_context_decoder = ExtractionContextDecoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=8,\n",
    "    hidden_dim=1024,\n",
    "    num_dec_layers=3\n",
    ").to(device)\n",
    "\n",
    "extractor = Extractor(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=8\n",
    ").to(device)\n",
    "\n",
    "global_context_encoder_ema = copy.deepcopy(global_context_encoder).to(device)\n",
    "extraction_context_decoder_ema = copy.deepcopy(extraction_context_decoder).to(device)\n",
    "extractor_ema = copy.deepcopy(extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c46ab25-936a-406d-a242-2389d4357f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = \\\n",
    "[par for par in global_context_encoder.parameters() if par.requires_grad] +\\\n",
    "[par for par in extraction_context_decoder.parameters() if par.requires_grad] +\\\n",
    "[par for par in extractor.parameters() if par.requires_grad]\n",
    "\n",
    "# Memo. make lse not trainable - too small VRAM\n",
    "#[par for par in local_sentence_encoder.parameters() if par.requires_grad] +\\\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model_parameters,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e2c1c8c-1303-4162-a31d-da3d8218e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class GovReportDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        split,\n",
    "        tokenizer,\n",
    "        output_type=\"packed\",\n",
    "        max_sequence_len=100,\n",
    "        max_sentence_num=500\n",
    "    ):\n",
    "        assert output_type in [\"packed\", \"padded\"], \"Invalid param:output_type\"\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.split = split\n",
    "        self.output_type = output_type\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.max_sentence_num = max_sentence_num\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            data_path = \"data/gov-report/train_GOVREPORT.jsonl\"\n",
    "        elif self.split == \"val\":\n",
    "            data_path = \"data/gov-report/val_GOVREPORT.jsonl\"\n",
    "        elif self.split == \"test\":\n",
    "            data_path = \"data/gov-report/test_GOVREPORT.jsonl\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid param:split\")\n",
    "            \n",
    "        with open(data_path, \"r\") as f:\n",
    "            self.data = [json.loads(line) for line in f.readlines()]\n",
    "            \n",
    "        self.data = [\n",
    "            x for x in self.data \n",
    "            if not (\n",
    "                \"text\" in x and len(x[\"text\"]) == 0\n",
    "                and \"indices\" in x and len(x[\"indices\"]) == 0\n",
    "                and \"summary\" in x and len(x[\"summary\"]) == 0\n",
    "                and \"score\" in x and len(x[\"score\"]) == 0\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def is_good_sentence(self, text):\n",
    "        # bad sentence :\n",
    "        #   1) empty sentence (after strip())\n",
    "        #   2) only a single character(such as \".\") (after strip())\n",
    "        if len(text.strip()) <= 1:\n",
    "            return False\n",
    "        \n",
    "        return True  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.data[idx]\n",
    "        sentences = doc[\"text\"]\n",
    "        gold_summary = doc[\"summary\"]\n",
    "        \n",
    "        # select only good sentences\n",
    "        sentences = [(idx, text) for idx, text in enumerate(sentences) if self.is_good_sentence(text)]\n",
    "        sent_idx_mapper = {old_sent_idx: new_sent_idx for new_sent_idx, (old_sent_idx, _) in enumerate(sentences)}\n",
    "        sentences = [text for _, text in sentences]\n",
    "        \n",
    "        # trim sentences to max_sentence_num\n",
    "        sentences = sentences[:self.max_sentence_num]\n",
    "        sentence_num = len(sentences)\n",
    "        \n",
    "        # tokenize sentences\n",
    "        tokenized = self.tokenizer(\n",
    "            sentences,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_sequence_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized.input_ids  # [sent_num, max_seq_len]\n",
    "        attention_mask = tokenized.attention_mask  # [sent_num, max_seq_len]\n",
    "        \n",
    "        if self.output_type == \"padded\":\n",
    "            # pad input_ids\n",
    "            padded_input_ids = torch.zeros(\n",
    "                [self.max_sentence_num, self.max_sequence_len],\n",
    "                dtype=input_ids.dtype,\n",
    "                device=input_ids.device\n",
    "            )\n",
    "            padded_input_ids[:sentence_num, :] = input_ids\n",
    "            input_ids = padded_input_ids  # [max_sent_num, max_seq_len]\n",
    "            \n",
    "            # pad attention_mask\n",
    "            padded_attention_mask = torch.zeros(\n",
    "                [self.max_sentence_num, self.max_sequence_len],\n",
    "                dtype=attention_mask.dtype,\n",
    "                device=attention_mask.device\n",
    "            )\n",
    "            padded_attention_mask[:sentence_num, :] = attention_mask\n",
    "            attention_mask = padded_attention_mask  # [max_sent_num, max_seq_len]\n",
    "        \n",
    "        # make (padded) doc_mask\n",
    "        doc_mask = torch.BoolTensor([False] * sentence_num + [True] * (self.max_sentence_num - sentence_num))\n",
    "        # Note. doc_mask is not required for packed output(since no sentence needs to be masked),\n",
    "        # but just calculate it for these two reasons\n",
    "        #   - eventually packed output needs to be converted to padded output after forwarding LSE,\n",
    "        #     and doc_mask needs to be built at that moment\n",
    "        #   - to make the shape of output be same with padded output\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            oracle_summaries = doc[\"indices\"]\n",
    "            oracle_summary_scores = doc[\"score\"]\n",
    "\n",
    "            # randomly select oracle summary\n",
    "            random_idx = np.random.choice(len(oracle_summaries))\n",
    "            oracle_summary = oracle_summaries[random_idx]\n",
    "            oracle_summary_score = oracle_summary_scores[random_idx]\n",
    "\n",
    "            # reset idxes in oracle_summary\n",
    "            oracle_summary = [sent_idx_mapper[old_sent_idx] for old_sent_idx in oracle_summary if old_sent_idx in sent_idx_mapper]\n",
    "            \n",
    "            # trim oracle_summary to max_sentence_num\n",
    "            oracle_summary = np.array(oracle_summary)\n",
    "            oracle_summary = oracle_summary[oracle_summary < sentence_num]\n",
    "\n",
    "            # shuffle oracle_summary\n",
    "            np.random.shuffle(oracle_summary)\n",
    "            oracle_summary = oracle_summary.tolist()\n",
    "\n",
    "            # pad oracle_summary (pad value = -1)\n",
    "            oracle_summary = torch.LongTensor(oracle_summary + [-1] * (self.max_sentence_num - len(oracle_summary)))\n",
    "            # Note. pad oracle_summary even for packed output,\n",
    "            # because eventually label need to be converted to padded version after forwarding LSE\n",
    "        \n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, oracle_summary, oracle_summary_score\n",
    "            \n",
    "        else: # val, test\n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary\n",
    "\n",
    "class collate_fn:\n",
    "    def __init__(self, split, output_type=\"packed\"):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"Invalid input:split\"\n",
    "        assert output_type in [\"packed\", \"padded\"], \"Invalid input:output_type\"\n",
    "        \n",
    "        self.split = split\n",
    "        self.output_type = output_type\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        if self.output_type == \"packed\":\n",
    "            input_ids = torch.cat([x[0] for x in batch], dim=0)  # tensor[sum(sent_nums), max_seq_len]\n",
    "            attention_mask = torch.cat([x[1] for x in batch], dim=0)  # tensor[sum(sent_nums), max_seq_len]\n",
    "            doc_mask = torch.stack([x[2] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            #label = torch.stack([x[3] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            sentence_num = [x[3] for x in batch]  # list[batch_size]  # same\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                oracle_summary = torch.stack([x[4] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "                oracle_summary_score = torch.tensor([x[5] for x in batch])  # tensor[batch_size]  # same\n",
    "            else: # val, test\n",
    "                sentences = [x[4] for x in batch]  # list[batch_size, sent_num]  # same\n",
    "                gold_summary = [x[5] for x in batch]  # list[batch_size, gold_summary_num]  # same\n",
    "                \n",
    "        elif self.output_type == \"padded\":\n",
    "            input_ids = torch.cat([x[0] for x in batch], dim=0)  # tensor[batch_size * max_sent_num, max_seq_len]\n",
    "            attention_mask = torch.cat([x[1] for x in batch], dim=0)  # tensor[batch_size * max_sent_num, max_seq_len]\n",
    "            doc_mask = torch.stack([x[2] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            #label = torch.stack([x[3] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            sentence_num = [x[3] for x in batch]  # list[batch_size]  # same\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                oracle_summary = torch.stack([x[4] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "                oracle_summary_score = torch.tensor([x[5] for x in batch])  # tensor[batch_size]  # same\n",
    "            else: # val, test\n",
    "                sentences = [x[4] for x in batch]  # list[batch_size, sent_num]  # same\n",
    "                gold_summary = [x[5] for x in batch]  # list[batch_size, gold_summary_num]  # same\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, oracle_summary, oracle_summary_score\n",
    "        \n",
    "        else: # val, test\n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5eac40e-8be4-47be-9c27-fabb2b2df15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset instances\n",
    "dataset_output_type = \"packed\"\n",
    "batch_size = 4  # use smaller batch_size for padded_output\n",
    "num_workers = 10\n",
    "\n",
    "train_dataset = GovReportDataset(\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    "    output_type=dataset_output_type,\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    max_sentence_num=max_sentence_num\n",
    ")\n",
    "\n",
    "val_dataset = GovReportDataset(\n",
    "    split=\"val\",\n",
    "    tokenizer=tokenizer,\n",
    "    output_type=dataset_output_type,\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    max_sentence_num=max_sentence_num\n",
    ")\n",
    "\n",
    "test_dataset = GovReportDataset(\n",
    "    split=\"test\",\n",
    "    tokenizer=tokenizer,\n",
    "    output_type=dataset_output_type,\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    max_sentence_num=max_sentence_num\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn(\n",
    "        split=\"train\",\n",
    "        output_type=dataset_output_type\n",
    "    ),\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn(\n",
    "        split=\"val\",\n",
    "        output_type=dataset_output_type\n",
    "    ),\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn(\n",
    "        split=\"val\",\n",
    "        output_type=dataset_output_type\n",
    "    ),\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f25d5ee5-d30b-4e8f-a7d2-b8e3e2146a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iteration(batch):\n",
    "    input_ids, attention_mask, doc_mask, sentence_num, oracle_summary, oracle_summary_score = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    doc_mask = doc_mask.to(device)  # tensor[batch_size, max_sent_num]\n",
    "    oracle_summary = oracle_summary.to(device)\n",
    "    oracle_summary_score = oracle_summary_score.to(device)\n",
    "    \n",
    "    batch_size = len(sentence_num)\n",
    "    \n",
    "    local_sen_embed = local_sentence_encoder(input_ids, attention_mask, sentence_num)  # [batch_size, max_sent_num, emb_dim]\n",
    "    global_context_embed = global_context_encoder(local_sen_embed, doc_mask, dropout_rate=0.1)  # [batch_size, max_sent_num, emb_dim]\n",
    "    \n",
    "    doc_mask_np = doc_mask.detach().cpu().numpy()  # np[batch_size, max_sent_num]\n",
    "    remaining_mask_np = np.ones_like( doc_mask_np ).astype( bool ) | doc_mask_np\n",
    "    extraction_mask_np = np.zeros_like( doc_mask_np ).astype( bool ) | doc_mask_np\n",
    "    \n",
    "    log_action_prob_list = []\n",
    "    log_stop_prob_list = []\n",
    "\n",
    "    done_list = []\n",
    "    extraction_context_embed = None\n",
    "    \n",
    "    for step in range(max_sentence_num):\n",
    "        remaining_mask = torch.from_numpy(remaining_mask_np).to(device)  # tensor[batch_size, max_sent_num]\n",
    "        extraction_mask = torch.from_numpy(extraction_mask_np).to(device)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        if step > 0: # if at least one sentence is selected\n",
    "            extraction_context_embed = extraction_context_decoder(local_sen_embed, remaining_mask, extraction_mask, dropout_rate=0.1)\n",
    "\n",
    "        sentence_scores, p_stop, _ = extractor(local_sen_embed, global_context_embed, extraction_context_embed, extraction_mask, dropout_rate=0.1)\n",
    "        # sentence_scores : tensor[batch_size, max_sent_num]\n",
    "        # p_stop : tensor[batch_size]\n",
    "        \n",
    "        p_stop = p_stop.unsqueeze(1)  # p_stop : tensor[batch_size, 1]\n",
    "        m_stop = Categorical(torch.cat([1 - p_stop, p_stop], dim=1))\n",
    "\n",
    "        # grep step-th summary sentence idx\n",
    "        summary_sent_idxs = oracle_summary[:, step]  # tensor[batch_size]\n",
    "\n",
    "        # find documents that all summary sentences are extracted(= padding is selected)\n",
    "        is_summarization_over = (summary_sent_idxs == -1)  # tensor[batch_size]\n",
    "\n",
    "        if len(done_list) > 0:\n",
    "            # is_just_stop : 이번 step에 막 summarization over 한거면 true, 아니면 false\n",
    "            is_just_stop = torch.logical_and(~done_list[-1], is_summarization_over)\n",
    "        else:\n",
    "            is_just_stop = is_summarization_over\n",
    "\n",
    "        if torch.all(is_summarization_over) and not torch.any(is_just_stop): # 모든 doc들의 summarization이 끝나고 1 step 진행한 경우 break\n",
    "            break\n",
    "\n",
    "        sentence_scores = sentence_scores.masked_fill(extraction_mask, 1e-12)\n",
    "        normalized_sentence_scores = sentence_scores / sentence_scores.sum(dim=1, keepdims=True)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        extracted_sentence_scores = normalized_sentence_scores[range(batch_size), summary_sent_idxs]  # tensor[batch_size]\n",
    "        log_action_prob = extracted_sentence_scores.masked_fill(is_summarization_over, 1.0).log()\n",
    "\n",
    "        log_stop_prob = m_stop.log_prob(is_summarization_over.long())\n",
    "        log_stop_prob = log_stop_prob.masked_fill(torch.logical_xor(is_summarization_over, is_just_stop), 0)\n",
    "\n",
    "        log_action_prob_list.append(log_action_prob.unsqueeze(1))\n",
    "        log_stop_prob_list.append(log_stop_prob.unsqueeze(1))\n",
    "        done_list.append(is_summarization_over)\n",
    "\n",
    "        for doc_idx, summary_sent_idx in enumerate(summary_sent_idxs.tolist()):\n",
    "            if summary_sent_idx != -1:\n",
    "                remaining_mask_np[doc_idx, summary_sent_idx] = False\n",
    "                extraction_mask_np[doc_idx, summary_sent_idx] = True\n",
    "\n",
    "    log_action_prob_list = torch.cat(log_action_prob_list, dim=1)\n",
    "    log_stop_prob_list = torch.cat(log_stop_prob_list, dim=1)\n",
    "    log_prob_list = log_action_prob_list + log_stop_prob_list\n",
    "    \n",
    "    log_prob_list = log_prob_list.sum(dim=1) / ((log_prob_list != 0).float().sum(dim=1))\n",
    "    \n",
    "    loss = (-log_prob_list * oracle_summary_score).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "170205a3-2a21-464c-9736-b8c57c3a1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stop_threshold = 0.6\n",
    "max_extracted_sentences_per_document = 22\n",
    "\n",
    "def validation_iteration(batch):\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary = batch\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        doc_mask = doc_mask.to(device)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        batch_size = len(sentence_num)\n",
    "\n",
    "        local_sent_embed = local_sentence_encoder(input_ids, attention_mask, sentence_num)  # [batch_size, max_sent_num, emb_dim]\n",
    "        global_context_embed = global_context_encoder_ema(local_sent_embed, doc_mask, dropout_rate=0.1)  # [batch_size, max_sent_num, emb_dim]\n",
    "        \n",
    "        doc_mask_np = doc_mask.detach().cpu().numpy()  # np[batch_size, max_sent_num]\n",
    "        \n",
    "        extracted_sents = []\n",
    "        extracted_sent_idxs = []\n",
    "        \n",
    "        sent_score_history = []\n",
    "        p_stop_history = []\n",
    "        \n",
    "        for doc_i in range(batch_size):\n",
    "            current_doc_mask_np = doc_mask_np[doc_i:doc_i + 1]\n",
    "            current_remaining_mask_np = np.ones_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            current_extraction_mask_np = np.zeros_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            \n",
    "            current_local_sent_embed = local_sent_embed[doc_i:doc_i + 1]\n",
    "            current_global_context_embed = global_context_embed[doc_i:doc_i + 1]\n",
    "            current_extraction_context_embed = None\n",
    "            \n",
    "            current_extracted_sent_idxs = []\n",
    "            extracted_sent_ngrams = set()\n",
    "            sent_score_history_for_doc_i = []\n",
    "            p_stop_history_for_doc_i = []\n",
    "            \n",
    "            for step in range(max_extracted_sentences_per_document + 1):\n",
    "                current_extraction_mask = torch.from_numpy(current_extraction_mask_np).to(device)\n",
    "                current_remaining_mask = torch.from_numpy(current_remaining_mask_np).to(device)\n",
    "                \n",
    "                if step > 0:\n",
    "                    current_extraction_context_embed = extraction_context_decoder_ema(\n",
    "                        current_local_sent_embed,\n",
    "                        current_remaining_mask,\n",
    "                        current_extraction_mask\n",
    "                    )\n",
    "                    \n",
    "                sent_scores, p_stop, _ = extractor_ema(\n",
    "                    current_local_sent_embed,\n",
    "                    current_global_context_embed,\n",
    "                    current_extraction_context_embed,\n",
    "                    current_extraction_mask\n",
    "                )\n",
    "                \n",
    "                p_stop = p_stop.item()\n",
    "                \n",
    "                p_stop_history_for_doc_i.append(p_stop)\n",
    "                sent_score_history_for_doc_i.append(sent_scores.detach().cpu().numpy())\n",
    "                \n",
    "                sent_scores = sent_scores.masked_fill(current_extraction_mask, 1e-12)\n",
    "                normalized_sent_scores = sent_scores / sent_scores.sum(dim=1, keepdims=True)\n",
    "                \n",
    "                is_summarization_over = (p_stop > p_stop_threshold)\n",
    "                \n",
    "                _, sorted_sent_idxs = normalized_sent_scores.sort(dim=1, descending=True)\n",
    "                sorted_sent_idxs = sorted_sent_idxs[0]\n",
    "                \n",
    "                # skip if selected sentence's ngrams are already in extracted sentences' ngram set\n",
    "                # (avoid redundancy)\n",
    "                extracted = False\n",
    "                for sent_idx in [x.item() for x in sorted_sent_idxs]:\n",
    "                    if sent_idx >= sentence_num[doc_i]:\n",
    "                        break\n",
    "                    \n",
    "                    selected_sent = sentences[doc_i][sent_idx]\n",
    "                    selected_sent_ngrams = get_ngram(selected_sent, n=3)\n",
    "                        \n",
    "                    if len(selected_sent_ngrams & extracted_sent_ngrams) < 1:\n",
    "                        extracted_sent_ngrams.update(selected_sent_ngrams)\n",
    "                        extracted = True\n",
    "                        break\n",
    "                \n",
    "                if not is_summarization_over and step < max_extracted_sentences_per_document and extracted:\n",
    "                    current_extracted_sent_idxs.append(sent_idx)\n",
    "                    current_extraction_mask_np[0, sent_idx] = True\n",
    "                    current_remaining_mask_np[0, sent_idx] = False\n",
    "                else:\n",
    "                    extracted_sents.append([sentences[doc_i][sent_idx] for sent_idx in current_extracted_sent_idxs])\n",
    "                    extracted_sent_idxs.append(current_extracted_sent_idxs)\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "            sent_score_history.append(sent_score_history_for_doc_i)\n",
    "            p_stop_history.append(p_stop_history_for_doc_i)\n",
    "        \n",
    "        scores = []\n",
    "        for doc_i in range(batch_size):\n",
    "            hyp = \"\\n\".join(extracted_sents[doc_i]).strip()\n",
    "            ref = \"\\n\".join(gold_summary[doc_i]).strip()\n",
    "\n",
    "            score = rouge_cal.score(hyp, ref)\n",
    "            scores.append((\n",
    "                score[\"rouge1\"].fmeasure,\n",
    "                score[\"rouge2\"].fmeasure,\n",
    "                score[\"rougeLsum\"].fmeasure\n",
    "            ))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d6aea2-28ef-42b7-a6ff-3ba3700f18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stop_threshold = 0.6\n",
    "max_extracted_sentences_per_document = 22\n",
    "\n",
    "def test_iteration(batch):\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary = batch\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        doc_mask = doc_mask.to(device)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        batch_size = len(sentence_num)\n",
    "\n",
    "        local_sent_embed = local_sentence_encoder(input_ids, attention_mask, sentence_num)  # [batch_size, max_sent_num, emb_dim]\n",
    "        global_context_embed = global_context_encoder(local_sent_embed, doc_mask, dropout_rate=0.1)  # [batch_size, max_sent_num, emb_dim]\n",
    "        \n",
    "        doc_mask_np = doc_mask.detach().cpu().numpy()  # np[batch_size, max_sent_num]\n",
    "        \n",
    "        extracted_sents = []\n",
    "        extracted_sent_idxs = []\n",
    "        \n",
    "        sent_score_history = []\n",
    "        p_stop_history = []\n",
    "        \n",
    "        for doc_i in range(batch_size):\n",
    "            current_doc_mask_np = doc_mask_np[doc_i:doc_i + 1]\n",
    "            current_remaining_mask_np = np.ones_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            current_extraction_mask_np = np.zeros_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            \n",
    "            current_local_sent_embed = local_sent_embed[doc_i:doc_i + 1]\n",
    "            current_global_context_embed = global_context_embed[doc_i:doc_i + 1]\n",
    "            current_extraction_context_embed = None\n",
    "            \n",
    "            current_extracted_sent_idxs = []\n",
    "            extracted_sent_ngrams = set()\n",
    "            sent_score_history_for_doc_i = []\n",
    "            p_stop_history_for_doc_i = []\n",
    "            \n",
    "            for step in range(max_extracted_sentences_per_document + 1):\n",
    "                current_extraction_mask = torch.from_numpy(current_extraction_mask_np).to(device)\n",
    "                current_remaining_mask = torch.from_numpy(current_remaining_mask_np).to(device)\n",
    "                \n",
    "                if step > 0:\n",
    "                    current_extraction_context_embed = extraction_context_decoder(\n",
    "                        current_local_sent_embed,\n",
    "                        current_remaining_mask,\n",
    "                        current_extraction_mask\n",
    "                    )\n",
    "                    \n",
    "                sent_scores, p_stop, _ = extractor(\n",
    "                    current_local_sent_embed,\n",
    "                    current_global_context_embed,\n",
    "                    current_extraction_context_embed,\n",
    "                    current_extraction_mask\n",
    "                )\n",
    "                \n",
    "                p_stop = p_stop.item()\n",
    "                \n",
    "                p_stop_history_for_doc_i.append(p_stop)\n",
    "                sent_score_history_for_doc_i.append(sent_scores.detach().cpu().numpy())\n",
    "                \n",
    "                sent_scores = sent_scores.masked_fill(current_extraction_mask, 1e-12)\n",
    "                normalized_sent_scores = sent_scores / sent_scores.sum(dim=1, keepdims=True)\n",
    "                \n",
    "                is_summarization_over = (p_stop > p_stop_threshold)\n",
    "                \n",
    "                _, sorted_sent_idxs = normalized_sent_scores.sort(dim=1, descending=True)\n",
    "                sorted_sent_idxs = sorted_sent_idxs[0]\n",
    "                \n",
    "                # skip if selected sentence's ngrams are already in extracted sentences' ngram set\n",
    "                # (avoid redundancy)\n",
    "                extracted = False\n",
    "                for sent_idx in [x.item() for x in sorted_sent_idxs]:\n",
    "                    if sent_idx >= sentence_num[doc_i]:\n",
    "                        break\n",
    "                    \n",
    "                    selected_sent = sentences[doc_i][sent_idx]\n",
    "                    selected_sent_ngrams = get_ngram(selected_sent, n=3)\n",
    "                        \n",
    "                    if len(selected_sent_ngrams & extracted_sent_ngrams) < 1:\n",
    "                        extracted_sent_ngrams.update(selected_sent_ngrams)\n",
    "                        extracted = True\n",
    "                        break\n",
    "                \n",
    "                if not is_summarization_over and step < max_extracted_sentences_per_document and extracted:\n",
    "                    current_extracted_sent_idxs.append(sent_idx)\n",
    "                    current_extraction_mask_np[0, sent_idx] = True\n",
    "                    current_remaining_mask_np[0, sent_idx] = False\n",
    "                else:\n",
    "                    extracted_sents.append([sentences[doc_i][sent_idx] for sent_idx in current_extracted_sent_idxs])\n",
    "                    extracted_sent_idxs.append(current_extracted_sent_idxs)\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "            sent_score_history.append(sent_score_history_for_doc_i)\n",
    "            p_stop_history.append(p_stop_history_for_doc_i)\n",
    "        \n",
    "        scores = []\n",
    "        for doc_i in range(batch_size):\n",
    "            hyp = \"\\n\".join(extracted_sents[doc_i]).strip()\n",
    "            ref = \"\\n\".join(gold_summary[doc_i]).strip()\n",
    "\n",
    "            score = rouge_cal.score(hyp, ref)\n",
    "            scores.append((\n",
    "                score[\"rouge1\"].fmeasure,\n",
    "                score[\"rouge2\"].fmeasure,\n",
    "                score[\"rougeLsum\"].fmeasure\n",
    "            ))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de954155-dfbc-46e4-a03a-bb59dfb9f459",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir: model/memsum_sbert/gov-report/2022-12-12-11-36-03\n",
      "\n",
      "\u001b[36m[Epoch 1]\u001b[0m\n",
      "\u001b[36m  [Training Batch 100 / 4380 (total_batch = 100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.247, elapsed time = (current) 2.058s / (mean) 2.144s\u001b[0m\n",
      "\u001b[36m  [Training Batch 200 / 4380 (total_batch = 200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.138, elapsed time = (current) 2.081s / (mean) 2.078s\u001b[0m\n",
      "\u001b[36m  [Training Batch 300 / 4380 (total_batch = 300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.158, elapsed time = (current) 1.669s / (mean) 2.056s\u001b[0m\n",
      "\u001b[36m  [Training Batch 400 / 4380 (total_batch = 400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.156, elapsed time = (current) 1.789s / (mean) 2.086s\u001b[0m\n",
      "\u001b[36m  [Training Batch 500 / 4380 (total_batch = 500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.098, elapsed time = (current) 1.524s / (mean) 2.016s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5424, rouge2 =  0.2033, rougeL =  0.5106, time = 11m 34.150s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/000501.0.542-0.203-0.511.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 600 / 4380 (total_batch = 600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.138, elapsed time = (current) 2.589s / (mean) 2.103s\u001b[0m\n",
      "\u001b[36m  [Training Batch 700 / 4380 (total_batch = 700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.095, elapsed time = (current) 2.392s / (mean) 2.077s\u001b[0m\n",
      "\u001b[36m  [Training Batch 800 / 4380 (total_batch = 800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.105, elapsed time = (current) 1.490s / (mean) 2.045s\u001b[0m\n",
      "\u001b[36m  [Training Batch 900 / 4380 (total_batch = 900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.049, elapsed time = (current) 1.933s / (mean) 2.078s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1000 / 4380 (total_batch = 1000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.110, elapsed time = (current) 2.313s / (mean) 2.156s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5586, rouge2 =  0.2151, rougeL =  0.5262, time = 11m 42.293s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/001001.0.559-0.215-0.526.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1100 / 4380 (total_batch = 1100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.119, elapsed time = (current) 1.759s / (mean) 2.081s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1200 / 4380 (total_batch = 1200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.028, elapsed time = (current) 2.021s / (mean) 2.026s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1300 / 4380 (total_batch = 1300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.036, elapsed time = (current) 2.161s / (mean) 2.016s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1400 / 4380 (total_batch = 1400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.123, elapsed time = (current) 1.853s / (mean) 2.127s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1500 / 4380 (total_batch = 1500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.131, elapsed time = (current) 2.023s / (mean) 2.153s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5642, rouge2 =  0.2220, rougeL =  0.5322, time = 11m 43.269s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/001501.0.564-0.222-0.532.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1600 / 4380 (total_batch = 1600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.072, elapsed time = (current) 2.140s / (mean) 2.036s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1700 / 4380 (total_batch = 1700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.072, elapsed time = (current) 2.637s / (mean) 2.116s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1800 / 4380 (total_batch = 1800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.085, elapsed time = (current) 2.155s / (mean) 2.109s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1900 / 4380 (total_batch = 1900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.065, elapsed time = (current) 1.720s / (mean) 2.096s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2000 / 4380 (total_batch = 2000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.079, elapsed time = (current) 2.198s / (mean) 2.112s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5685, rouge2 =  0.2272, rougeL =  0.5363, time = 11m 57.557s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/002001.0.568-0.227-0.536.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2100 / 4380 (total_batch = 2100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.063, elapsed time = (current) 2.063s / (mean) 2.060s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2200 / 4380 (total_batch = 2200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.036, elapsed time = (current) 2.237s / (mean) 2.075s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2300 / 4380 (total_batch = 2300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.075, elapsed time = (current) 2.408s / (mean) 2.106s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2400 / 4380 (total_batch = 2400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.059, elapsed time = (current) 1.612s / (mean) 2.086s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2500 / 4380 (total_batch = 2500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.057, elapsed time = (current) 2.548s / (mean) 2.066s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5702, rouge2 =  0.2280, rougeL =  0.5382, time = 12m 18.620s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/002501.0.570-0.228-0.538.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2600 / 4380 (total_batch = 2600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.038, elapsed time = (current) 2.225s / (mean) 2.086s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2700 / 4380 (total_batch = 2700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.056, elapsed time = (current) 2.479s / (mean) 2.068s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2800 / 4380 (total_batch = 2800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.032, elapsed time = (current) 1.615s / (mean) 2.011s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2900 / 4380 (total_batch = 2900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.014, elapsed time = (current) 1.628s / (mean) 2.065s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3000 / 4380 (total_batch = 3000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.058, elapsed time = (current) 2.464s / (mean) 2.018s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5700, rouge2 =  0.2248, rougeL =  0.5374, time = 11m 48.761s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/003001.0.570-0.225-0.537.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3100 / 4380 (total_batch = 3100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.051, elapsed time = (current) 2.565s / (mean) 2.043s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3200 / 4380 (total_batch = 3200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.997, elapsed time = (current) 2.129s / (mean) 2.014s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3300 / 4380 (total_batch = 3300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.044, elapsed time = (current) 1.879s / (mean) 2.037s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3400 / 4380 (total_batch = 3400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.055, elapsed time = (current) 1.730s / (mean) 2.096s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3500 / 4380 (total_batch = 3500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.028, elapsed time = (current) 1.896s / (mean) 2.083s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5742, rouge2 =  0.2330, rougeL =  0.5419, time = 11m 32.236s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/003501.0.574-0.233-0.542.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3600 / 4380 (total_batch = 3600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.041, elapsed time = (current) 1.690s / (mean) 2.078s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3700 / 4380 (total_batch = 3700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.018, elapsed time = (current) 2.515s / (mean) 2.043s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3800 / 4380 (total_batch = 3800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.027, elapsed time = (current) 1.744s / (mean) 2.092s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3900 / 4380 (total_batch = 3900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.050, elapsed time = (current) 2.060s / (mean) 2.102s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4000 / 4380 (total_batch = 4000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.028, elapsed time = (current) 1.959s / (mean) 2.090s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5737, rouge2 =  0.2325, rougeL =  0.5412, time = 11m 31.123s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/004001.0.574-0.233-0.541.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 4100 / 4380 (total_batch = 4100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.957, elapsed time = (current) 2.313s / (mean) 1.992s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4200 / 4380 (total_batch = 4200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.060, elapsed time = (current) 2.379s / (mean) 2.085s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4300 / 4380 (total_batch = 4300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.993, elapsed time = (current) 2.247s / (mean) 2.082s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5743, rouge2 =  0.2340, rougeL =  0.5417, time = 11m 29.634s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/004381.0.574-0.234-0.542.pt\u001b[0m\n",
      "\u001b[36m[Epoch 2]\u001b[0m\n",
      "\u001b[36m  [Training Batch 20 / 4380 (total_batch = 4400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  0.578, elapsed time = (current) 2.179s / (mean) 0.404s\u001b[0m\n",
      "\u001b[36m  [Training Batch 120 / 4380 (total_batch = 4500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.946, elapsed time = (current) 2.146s / (mean) 2.030s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5751, rouge2 =  0.2344, rougeL =  0.5427, time = 11m 33.582s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/004501.0.575-0.234-0.543.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 220 / 4380 (total_batch = 4600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.981, elapsed time = (current) 2.166s / (mean) 2.054s\u001b[0m\n",
      "\u001b[36m  [Training Batch 320 / 4380 (total_batch = 4700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.971, elapsed time = (current) 2.546s / (mean) 2.057s\u001b[0m\n",
      "\u001b[36m  [Training Batch 420 / 4380 (total_batch = 4800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.999, elapsed time = (current) 1.694s / (mean) 2.089s\u001b[0m\n",
      "\u001b[36m  [Training Batch 520 / 4380 (total_batch = 4900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.007, elapsed time = (current) 2.400s / (mean) 2.084s\u001b[0m\n",
      "\u001b[36m  [Training Batch 620 / 4380 (total_batch = 5000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.936, elapsed time = (current) 1.998s / (mean) 2.012s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5740, rouge2 =  0.2326, rougeL =  0.5413, time = 12m 3.836s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/005001.0.574-0.233-0.541.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 720 / 4380 (total_batch = 5100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.982, elapsed time = (current) 1.886s / (mean) 2.071s\u001b[0m\n",
      "\u001b[36m  [Training Batch 820 / 4380 (total_batch = 5200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.008, elapsed time = (current) 2.160s / (mean) 2.090s\u001b[0m\n",
      "\u001b[36m  [Training Batch 920 / 4380 (total_batch = 5300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.996, elapsed time = (current) 1.727s / (mean) 2.552s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1020 / 4380 (total_batch = 5400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.968, elapsed time = (current) 1.957s / (mean) 2.038s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1120 / 4380 (total_batch = 5500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.929, elapsed time = (current) 1.495s / (mean) 2.001s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5746, rouge2 =  0.2341, rougeL =  0.5421, time = 11m 32.450s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/005501.0.575-0.234-0.542.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1220 / 4380 (total_batch = 5600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.997, elapsed time = (current) 2.215s / (mean) 2.099s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1320 / 4380 (total_batch = 5700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.004, elapsed time = (current) 1.824s / (mean) 2.154s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1420 / 4380 (total_batch = 5800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.916, elapsed time = (current) 1.709s / (mean) 2.048s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1520 / 4380 (total_batch = 5900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.012, elapsed time = (current) 2.148s / (mean) 2.114s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1620 / 4380 (total_batch = 6000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.043, elapsed time = (current) 1.965s / (mean) 2.194s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5757, rouge2 =  0.2349, rougeL =  0.5429, time = 11m 39.093s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/006001.0.576-0.235-0.543.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1720 / 4380 (total_batch = 6100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.976, elapsed time = (current) 1.804s / (mean) 2.002s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1820 / 4380 (total_batch = 6200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.031, elapsed time = (current) 1.959s / (mean) 2.112s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1920 / 4380 (total_batch = 6300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.010, elapsed time = (current) 1.807s / (mean) 2.119s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2020 / 4380 (total_batch = 6400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.995, elapsed time = (current) 1.727s / (mean) 2.101s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2120 / 4380 (total_batch = 6500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.945, elapsed time = (current) 1.788s / (mean) 2.007s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5776, rouge2 =  0.2387, rougeL =  0.5447, time = 11m 27.492s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/006501.0.578-0.239-0.545.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2220 / 4380 (total_batch = 6600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.014, elapsed time = (current) 2.380s / (mean) 2.090s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2320 / 4380 (total_batch = 6700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.924, elapsed time = (current) 1.920s / (mean) 2.023s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2420 / 4380 (total_batch = 6800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.002, elapsed time = (current) 1.855s / (mean) 2.091s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2520 / 4380 (total_batch = 6900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.962, elapsed time = (current) 1.797s / (mean) 2.078s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2620 / 4380 (total_batch = 7000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.936, elapsed time = (current) 2.452s / (mean) 2.049s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5768, rouge2 =  0.2376, rougeL =  0.5441, time = 11m 39.223s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/007001.0.577-0.238-0.544.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2720 / 4380 (total_batch = 7100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.970, elapsed time = (current) 2.127s / (mean) 2.041s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2820 / 4380 (total_batch = 7200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.957, elapsed time = (current) 2.226s / (mean) 2.069s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2920 / 4380 (total_batch = 7300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.952, elapsed time = (current) 1.560s / (mean) 2.021s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3020 / 4380 (total_batch = 7400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.006, elapsed time = (current) 2.086s / (mean) 2.034s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3120 / 4380 (total_batch = 7500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.962, elapsed time = (current) 1.753s / (mean) 2.098s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5764, rouge2 =  0.2374, rougeL =  0.5437, time = 11m 28.257s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/007501.0.576-0.237-0.544.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3220 / 4380 (total_batch = 7600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.915, elapsed time = (current) 1.767s / (mean) 2.012s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3320 / 4380 (total_batch = 7700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.041, elapsed time = (current) 2.391s / (mean) 2.128s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3420 / 4380 (total_batch = 7800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.000, elapsed time = (current) 1.756s / (mean) 2.068s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3520 / 4380 (total_batch = 7900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.961, elapsed time = (current) 1.905s / (mean) 2.026s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3620 / 4380 (total_batch = 8000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.013, elapsed time = (current) 1.900s / (mean) 2.022s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5783, rouge2 =  0.2384, rougeL =  0.5458, time = 11m 24.689s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/008001.0.578-0.238-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3720 / 4380 (total_batch = 8100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.946, elapsed time = (current) 1.886s / (mean) 2.019s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3820 / 4380 (total_batch = 8200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.931, elapsed time = (current) 1.995s / (mean) 2.017s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3920 / 4380 (total_batch = 8300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.989, elapsed time = (current) 2.497s / (mean) 2.082s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4020 / 4380 (total_batch = 8400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.928, elapsed time = (current) 1.676s / (mean) 2.075s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4120 / 4380 (total_batch = 8500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.962, elapsed time = (current) 2.294s / (mean) 2.072s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5773, rouge2 =  0.2379, rougeL =  0.5445, time = 11m 20.933s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/008501.0.577-0.238-0.545.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 4220 / 4380 (total_batch = 8600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.928, elapsed time = (current) 1.960s / (mean) 2.017s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4320 / 4380 (total_batch = 8700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.989, elapsed time = (current) 1.231s / (mean) 2.093s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5796, rouge2 =  0.2404, rougeL =  0.5469, time = 11m 18.491s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/008761.0.580-0.240-0.547.pt\u001b[0m\n",
      "\u001b[36m[Epoch 3]\u001b[0m\n",
      "\u001b[36m  [Training Batch 40 / 4380 (total_batch = 8800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  1.170, elapsed time = (current) 2.006s / (mean) 0.787s\u001b[0m\n",
      "\u001b[36m  [Training Batch 140 / 4380 (total_batch = 8900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.889, elapsed time = (current) 2.398s / (mean) 2.031s\u001b[0m\n",
      "\u001b[36m  [Training Batch 240 / 4380 (total_batch = 9000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.923, elapsed time = (current) 1.839s / (mean) 2.038s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5792, rouge2 =  0.2404, rougeL =  0.5468, time = 11m 20.471s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/009001.0.579-0.240-0.547.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 340 / 4380 (total_batch = 9100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.891, elapsed time = (current) 2.008s / (mean) 2.031s\u001b[0m\n",
      "\u001b[36m  [Training Batch 440 / 4380 (total_batch = 9200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.930, elapsed time = (current) 1.530s / (mean) 2.100s\u001b[0m\n",
      "\u001b[36m  [Training Batch 540 / 4380 (total_batch = 9300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.861, elapsed time = (current) 2.096s / (mean) 2.000s\u001b[0m\n",
      "\u001b[36m  [Training Batch 640 / 4380 (total_batch = 9400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.885, elapsed time = (current) 1.662s / (mean) 2.004s\u001b[0m\n",
      "\u001b[36m  [Training Batch 740 / 4380 (total_batch = 9500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.915, elapsed time = (current) 1.770s / (mean) 2.070s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5785, rouge2 =  0.2407, rougeL =  0.5460, time = 11m 17.954s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/009501.0.579-0.241-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 840 / 4380 (total_batch = 9600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.866, elapsed time = (current) 2.017s / (mean) 2.002s\u001b[0m\n",
      "\u001b[36m  [Training Batch 940 / 4380 (total_batch = 9700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.947, elapsed time = (current) 2.259s / (mean) 2.075s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1040 / 4380 (total_batch = 9800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.963, elapsed time = (current) 2.164s / (mean) 2.050s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1140 / 4380 (total_batch = 9900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.906, elapsed time = (current) 2.125s / (mean) 2.063s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1240 / 4380 (total_batch = 10000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.889, elapsed time = (current) 1.613s / (mean) 2.009s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5762, rouge2 =  0.2396, rougeL =  0.5438, time = 11m 15.128s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/010001.0.576-0.240-0.544.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1340 / 4380 (total_batch = 10100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.904, elapsed time = (current) 1.533s / (mean) 2.026s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1440 / 4380 (total_batch = 10200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.925, elapsed time = (current) 2.020s / (mean) 2.063s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1540 / 4380 (total_batch = 10300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.908, elapsed time = (current) 2.061s / (mean) 2.105s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1640 / 4380 (total_batch = 10400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.916, elapsed time = (current) 2.510s / (mean) 2.127s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1740 / 4380 (total_batch = 10500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.901, elapsed time = (current) 2.457s / (mean) 2.106s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5777, rouge2 =  0.2391, rougeL =  0.5448, time = 11m 44.592s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/010501.0.578-0.239-0.545.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1840 / 4380 (total_batch = 10600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.957, elapsed time = (current) 2.469s / (mean) 2.093s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1940 / 4380 (total_batch = 10700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.911, elapsed time = (current) 1.967s / (mean) 2.065s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2040 / 4380 (total_batch = 10800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.922, elapsed time = (current) 1.786s / (mean) 2.102s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2140 / 4380 (total_batch = 10900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.922, elapsed time = (current) 1.691s / (mean) 2.500s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2240 / 4380 (total_batch = 11000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.883, elapsed time = (current) 2.405s / (mean) 2.024s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5786, rouge2 =  0.2406, rougeL =  0.5461, time = 11m 38.026s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/011001.0.579-0.241-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2340 / 4380 (total_batch = 11100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.854, elapsed time = (current) 2.029s / (mean) 2.039s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2440 / 4380 (total_batch = 11200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.887, elapsed time = (current) 1.886s / (mean) 2.038s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2540 / 4380 (total_batch = 11300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.897, elapsed time = (current) 2.117s / (mean) 2.053s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2640 / 4380 (total_batch = 11400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.863, elapsed time = (current) 2.463s / (mean) 2.030s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2740 / 4380 (total_batch = 11500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.960, elapsed time = (current) 2.234s / (mean) 2.118s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5784, rouge2 =  0.2405, rougeL =  0.5458, time = 12m 10.127s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/011501.0.578-0.240-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2840 / 4380 (total_batch = 11600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.909, elapsed time = (current) 2.023s / (mean) 2.092s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2940 / 4380 (total_batch = 11700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.932, elapsed time = (current) 2.074s / (mean) 2.093s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3040 / 4380 (total_batch = 11800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.915, elapsed time = (current) 2.162s / (mean) 2.062s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3140 / 4380 (total_batch = 11900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.893, elapsed time = (current) 1.984s / (mean) 2.096s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3240 / 4380 (total_batch = 12000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.936, elapsed time = (current) 1.865s / (mean) 2.112s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5789, rouge2 =  0.2399, rougeL =  0.5461, time = 12m 12.601s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/012001.0.579-0.240-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3340 / 4380 (total_batch = 12100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.870, elapsed time = (current) 1.488s / (mean) 2.009s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3440 / 4380 (total_batch = 12200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.948, elapsed time = (current) 1.756s / (mean) 2.058s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3540 / 4380 (total_batch = 12300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.945, elapsed time = (current) 2.050s / (mean) 2.059s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3640 / 4380 (total_batch = 12400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.926, elapsed time = (current) 2.397s / (mean) 2.063s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3740 / 4380 (total_batch = 12500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.869, elapsed time = (current) 1.840s / (mean) 1.974s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5792, rouge2 =  0.2404, rougeL =  0.5464, time = 12m 5.223s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/012501.0.579-0.240-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3840 / 4380 (total_batch = 12600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.875, elapsed time = (current) 1.582s / (mean) 2.029s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3940 / 4380 (total_batch = 12700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.875, elapsed time = (current) 1.811s / (mean) 2.033s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4040 / 4380 (total_batch = 12800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.915, elapsed time = (current) 1.978s / (mean) 2.041s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4140 / 4380 (total_batch = 12900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.910, elapsed time = (current) 2.224s / (mean) 2.062s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4240 / 4380 (total_batch = 13000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.925, elapsed time = (current) 1.528s / (mean) 2.066s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5808, rouge2 =  0.2415, rougeL =  0.5481, time = 12m 7.997s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/013001.0.581-0.242-0.548.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 4340 / 4380 (total_batch = 13100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.946, elapsed time = (current) 2.000s / (mean) 2.083s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5788, rouge2 =  0.2405, rougeL =  0.5461, time = 12m 14.297s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/013141.0.579-0.241-0.546.pt\u001b[0m\n",
      "\u001b[36m[Epoch 4]\u001b[0m\n",
      "\u001b[36m  [Training Batch 60 / 4380 (total_batch = 13200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  1.672, elapsed time = (current) 1.361s / (mean) 1.181s\u001b[0m\n",
      "\u001b[36m  [Training Batch 160 / 4380 (total_batch = 13300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.862, elapsed time = (current) 1.828s / (mean) 2.068s\u001b[0m\n",
      "\u001b[36m  [Training Batch 260 / 4380 (total_batch = 13400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.850, elapsed time = (current) 1.811s / (mean) 2.028s\u001b[0m\n",
      "\u001b[36m  [Training Batch 360 / 4380 (total_batch = 13500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.826, elapsed time = (current) 1.861s / (mean) 2.063s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5786, rouge2 =  0.2410, rougeL =  0.5458, time = 12m 5.951s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/013501.0.579-0.241-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 460 / 4380 (total_batch = 13600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.820, elapsed time = (current) 1.371s / (mean) 2.014s\u001b[0m\n",
      "\u001b[36m  [Training Batch 560 / 4380 (total_batch = 13700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.851, elapsed time = (current) 1.826s / (mean) 2.046s\u001b[0m\n",
      "\u001b[36m  [Training Batch 660 / 4380 (total_batch = 13800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.814, elapsed time = (current) 2.333s / (mean) 2.054s\u001b[0m\n",
      "\u001b[36m  [Training Batch 760 / 4380 (total_batch = 13900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.824, elapsed time = (current) 2.305s / (mean) 2.090s\u001b[0m\n",
      "\u001b[36m  [Training Batch 860 / 4380 (total_batch = 14000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.830, elapsed time = (current) 1.883s / (mean) 2.073s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5781, rouge2 =  0.2407, rougeL =  0.5455, time = 12m 2.981s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/014001.0.578-0.241-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 960 / 4380 (total_batch = 14100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.820, elapsed time = (current) 1.735s / (mean) 2.061s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1060 / 4380 (total_batch = 14200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.850, elapsed time = (current) 2.097s / (mean) 2.130s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1160 / 4380 (total_batch = 14300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.853, elapsed time = (current) 1.971s / (mean) 2.069s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1260 / 4380 (total_batch = 14400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.810, elapsed time = (current) 2.342s / (mean) 2.044s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1360 / 4380 (total_batch = 14500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.822, elapsed time = (current) 2.711s / (mean) 2.041s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5782, rouge2 =  0.2398, rougeL =  0.5458, time = 12m 9.048s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/014501.0.578-0.240-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1460 / 4380 (total_batch = 14600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.832, elapsed time = (current) 2.479s / (mean) 2.051s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1560 / 4380 (total_batch = 14700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.846, elapsed time = (current) 1.982s / (mean) 2.075s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1660 / 4380 (total_batch = 14800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.898, elapsed time = (current) 2.212s / (mean) 2.079s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1760 / 4380 (total_batch = 14900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.904, elapsed time = (current) 1.817s / (mean) 2.101s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1860 / 4380 (total_batch = 15000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.880, elapsed time = (current) 1.913s / (mean) 2.073s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5784, rouge2 =  0.2409, rougeL =  0.5465, time = 11m 52.322s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/015001.0.578-0.241-0.547.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1960 / 4380 (total_batch = 15100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.805, elapsed time = (current) 2.295s / (mean) 2.058s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2060 / 4380 (total_batch = 15200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.856, elapsed time = (current) 1.697s / (mean) 2.096s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2160 / 4380 (total_batch = 15300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.815, elapsed time = (current) 1.545s / (mean) 2.047s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2260 / 4380 (total_batch = 15400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.815, elapsed time = (current) 2.730s / (mean) 2.038s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2360 / 4380 (total_batch = 15500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.808, elapsed time = (current) 2.103s / (mean) 2.012s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5781, rouge2 =  0.2399, rougeL =  0.5452, time = 11m 48.639s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/015501.0.578-0.240-0.545.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2460 / 4380 (total_batch = 15600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.807, elapsed time = (current) 2.216s / (mean) 2.088s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2560 / 4380 (total_batch = 15700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.861, elapsed time = (current) 2.433s / (mean) 2.076s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2660 / 4380 (total_batch = 15800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.855, elapsed time = (current) 2.000s / (mean) 2.042s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2760 / 4380 (total_batch = 15900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.871, elapsed time = (current) 1.796s / (mean) 2.048s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2860 / 4380 (total_batch = 16000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.843, elapsed time = (current) 1.910s / (mean) 2.011s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5812, rouge2 =  0.2421, rougeL =  0.5487, time = 11m 46.121s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/016001.0.581-0.242-0.549.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2960 / 4380 (total_batch = 16100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.863, elapsed time = (current) 1.947s / (mean) 2.054s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3060 / 4380 (total_batch = 16200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.812, elapsed time = (current) 1.743s / (mean) 2.047s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3160 / 4380 (total_batch = 16300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.847, elapsed time = (current) 2.616s / (mean) 2.051s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3260 / 4380 (total_batch = 16400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.865, elapsed time = (current) 2.187s / (mean) 2.087s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3360 / 4380 (total_batch = 16500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.910, elapsed time = (current) 2.642s / (mean) 2.138s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5784, rouge2 =  0.2394, rougeL =  0.5457, time = 11m 40.995s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/016501.0.578-0.239-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3460 / 4380 (total_batch = 16600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.847, elapsed time = (current) 2.060s / (mean) 2.082s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3560 / 4380 (total_batch = 16700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.868, elapsed time = (current) 2.958s / (mean) 2.054s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3660 / 4380 (total_batch = 16800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.857, elapsed time = (current) 2.160s / (mean) 2.062s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3760 / 4380 (total_batch = 16900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.831, elapsed time = (current) 2.205s / (mean) 2.064s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3860 / 4380 (total_batch = 17000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.843, elapsed time = (current) 2.193s / (mean) 2.060s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5792, rouge2 =  0.2408, rougeL =  0.5466, time = 11m 51.965s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/017001.0.579-0.241-0.547.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3960 / 4380 (total_batch = 17100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.831, elapsed time = (current) 1.983s / (mean) 2.028s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4060 / 4380 (total_batch = 17200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.857, elapsed time = (current) 2.609s / (mean) 2.039s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4160 / 4380 (total_batch = 17300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.847, elapsed time = (current) 1.846s / (mean) 2.057s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4260 / 4380 (total_batch = 17400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.847, elapsed time = (current) 2.105s / (mean) 2.060s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4360 / 4380 (total_batch = 17500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.872, elapsed time = (current) 2.227s / (mean) 2.058s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5811, rouge2 =  0.2424, rougeL =  0.5487, time = 11m 36.902s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/017501.0.581-0.242-0.549.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5807, rouge2 =  0.2408, rougeL =  0.5481, time = 11m 42.346s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/017521.0.581-0.241-0.548.pt\u001b[0m\n",
      "\u001b[36m[Epoch 5]\u001b[0m\n",
      "\u001b[36m  [Training Batch 80 / 4380 (total_batch = 17600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.170, elapsed time = (current) 2.212s / (mean) 1.619s\u001b[0m\n",
      "\u001b[36m  [Training Batch 180 / 4380 (total_batch = 17700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.815, elapsed time = (current) 2.170s / (mean) 2.075s\u001b[0m\n",
      "\u001b[36m  [Training Batch 280 / 4380 (total_batch = 17800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.770, elapsed time = (current) 1.909s / (mean) 2.042s\u001b[0m\n",
      "\u001b[36m  [Training Batch 380 / 4380 (total_batch = 17900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.773, elapsed time = (current) 1.505s / (mean) 2.063s\u001b[0m\n",
      "\u001b[36m  [Training Batch 480 / 4380 (total_batch = 18000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.782, elapsed time = (current) 2.227s / (mean) 2.113s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5793, rouge2 =  0.2411, rougeL =  0.5469, time = 11m 40.675s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/018001.0.579-0.241-0.547.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 580 / 4380 (total_batch = 18100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.763, elapsed time = (current) 2.198s / (mean) 2.053s\u001b[0m\n",
      "\u001b[36m  [Training Batch 680 / 4380 (total_batch = 18200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.804, elapsed time = (current) 2.256s / (mean) 2.114s\u001b[0m\n",
      "\u001b[36m  [Training Batch 780 / 4380 (total_batch = 18300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.785, elapsed time = (current) 2.208s / (mean) 2.069s\u001b[0m\n",
      "\u001b[36m  [Training Batch 880 / 4380 (total_batch = 18400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.789, elapsed time = (current) 1.916s / (mean) 2.063s\u001b[0m\n",
      "\u001b[36m  [Training Batch 980 / 4380 (total_batch = 18500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.764, elapsed time = (current) 1.631s / (mean) 2.086s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5788, rouge2 =  0.2395, rougeL =  0.5463, time = 11m 46.857s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/018501.0.579-0.240-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1080 / 4380 (total_batch = 18600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.759, elapsed time = (current) 2.433s / (mean) 2.042s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1180 / 4380 (total_batch = 18700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.751, elapsed time = (current) 1.386s / (mean) 2.035s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1280 / 4380 (total_batch = 18800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.789, elapsed time = (current) 2.053s / (mean) 2.088s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1380 / 4380 (total_batch = 18900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.778, elapsed time = (current) 1.959s / (mean) 2.045s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1480 / 4380 (total_batch = 19000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.771, elapsed time = (current) 2.249s / (mean) 2.037s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5800, rouge2 =  0.2402, rougeL =  0.5472, time = 11m 45.548s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/019001.0.580-0.240-0.547.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 1580 / 4380 (total_batch = 19100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.773, elapsed time = (current) 2.611s / (mean) 2.089s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1680 / 4380 (total_batch = 19200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.759, elapsed time = (current) 1.503s / (mean) 2.015s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1780 / 4380 (total_batch = 19300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.727, elapsed time = (current) 2.106s / (mean) 1.996s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1880 / 4380 (total_batch = 19400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.789, elapsed time = (current) 1.701s / (mean) 2.076s\u001b[0m\n",
      "\u001b[36m  [Training Batch 1980 / 4380 (total_batch = 19500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.784, elapsed time = (current) 1.868s / (mean) 2.051s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5765, rouge2 =  0.2389, rougeL =  0.5439, time = 11m 38.717s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/019501.0.576-0.239-0.544.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2080 / 4380 (total_batch = 19600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.746, elapsed time = (current) 2.096s / (mean) 2.029s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2180 / 4380 (total_batch = 19700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.763, elapsed time = (current) 2.279s / (mean) 2.068s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2280 / 4380 (total_batch = 19800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.793, elapsed time = (current) 2.348s / (mean) 2.046s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2380 / 4380 (total_batch = 19900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.796, elapsed time = (current) 1.867s / (mean) 2.099s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2480 / 4380 (total_batch = 20000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.784, elapsed time = (current) 2.387s / (mean) 2.062s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5771, rouge2 =  0.2366, rougeL =  0.5442, time = 11m 32.635s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/020001.0.577-0.237-0.544.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 2580 / 4380 (total_batch = 20100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.772, elapsed time = (current) 1.781s / (mean) 2.045s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2680 / 4380 (total_batch = 20200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.710, elapsed time = (current) 2.283s / (mean) 2.009s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2780 / 4380 (total_batch = 20300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.802, elapsed time = (current) 1.736s / (mean) 2.071s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2880 / 4380 (total_batch = 20400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.770, elapsed time = (current) 2.425s / (mean) 2.031s\u001b[0m\n",
      "\u001b[36m  [Training Batch 2980 / 4380 (total_batch = 20500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.757, elapsed time = (current) 1.971s / (mean) 2.023s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5780, rouge2 =  0.2396, rougeL =  0.5454, time = 11m 32.404s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/020501.0.578-0.240-0.545.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3080 / 4380 (total_batch = 20600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.778, elapsed time = (current) 2.657s / (mean) 2.078s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3180 / 4380 (total_batch = 20700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.860, elapsed time = (current) 1.964s / (mean) 2.094s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3280 / 4380 (total_batch = 20800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.787, elapsed time = (current) 1.829s / (mean) 2.071s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3380 / 4380 (total_batch = 20900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.769, elapsed time = (current) 2.633s / (mean) 2.063s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3480 / 4380 (total_batch = 21000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.778, elapsed time = (current) 2.369s / (mean) 2.116s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5764, rouge2 =  0.2374, rougeL =  0.5438, time = 11m 26.492s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/021001.0.576-0.237-0.544.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 3580 / 4380 (total_batch = 21100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.752, elapsed time = (current) 1.752s / (mean) 2.085s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3680 / 4380 (total_batch = 21200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.796, elapsed time = (current) 2.302s / (mean) 2.092s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3780 / 4380 (total_batch = 21300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.787, elapsed time = (current) 2.450s / (mean) 2.045s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3880 / 4380 (total_batch = 21400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.845, elapsed time = (current) 1.869s / (mean) 2.139s\u001b[0m\n",
      "\u001b[36m  [Training Batch 3980 / 4380 (total_batch = 21500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.796, elapsed time = (current) 1.786s / (mean) 2.121s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5784, rouge2 =  0.2402, rougeL =  0.5456, time = 11m 40.069s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/021501.0.578-0.240-0.546.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 4080 / 4380 (total_batch = 21600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.788, elapsed time = (current) 2.694s / (mean) 2.102s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4180 / 4380 (total_batch = 21700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.786, elapsed time = (current) 2.024s / (mean) 2.068s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4280 / 4380 (total_batch = 21800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.762, elapsed time = (current) 2.132s / (mean) 2.047s\u001b[0m\n",
      "\u001b[36m  [Training Batch 4380 / 4380 (total_batch = 21900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.762, elapsed time = (current) 0.673s / (mean) 1.975s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.5758, rouge2 =  0.2387, rougeL =  0.5429, time = 11m 29.427s => model/memsum_sbert/gov-report/2022-12-12-11-36-03/021901.0.576-0.239-0.543.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "cur_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "model_dir = os.path.join(\"model/memsum_sbert/gov-report\", cur_time)\n",
    "\n",
    "epochs = 5  # 50\n",
    "print_every = 100  # 100\n",
    "validate_every = 500  # 100\n",
    "early_stop = False  # False\n",
    "\n",
    "print(f\"model dir: {model_dir}\")\n",
    "print()\n",
    "\n",
    "total_train_batch = 0\n",
    "for epoch in range(epochs):\n",
    "    print.info(f\"[Epoch {epoch + 1}]\")\n",
    "    running_loss = 0\n",
    "    total_train_elapsed_time = 0\n",
    "    \n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        train_start_time = time.time()\n",
    "        total_train_batch += 1\n",
    "        \n",
    "        loss = train_iteration(train_batch)\n",
    "        running_loss += loss\n",
    "        \n",
    "        #scheduler.step()\n",
    "        \n",
    "        update_moving_average(global_context_encoder_ema, global_context_encoder)\n",
    "        update_moving_average(extraction_context_decoder_ema, extraction_context_decoder)\n",
    "        update_moving_average(extractor_ema, extractor)\n",
    "        \n",
    "        train_elapsed_time = time.time() - train_start_time\n",
    "        total_train_elapsed_time += train_elapsed_time\n",
    "        \n",
    "        if total_train_batch % print_every == 0:\n",
    "            print.info(f\"  [Training Batch {i + 1} / {len(train_dataloader)} (total_batch = {total_train_batch})]\")\n",
    "            #current_lr = optimizer.param_groups[0]['lr']\n",
    "            print.info(f\"    training : loss = {running_loss / print_every : .3f}, elapsed time = (current) {print_time(train_elapsed_time)} / (mean) {print_time(total_train_elapsed_time / print_every)}\")\n",
    "            running_loss = 0\n",
    "            total_train_elapsed_time = 0\n",
    "                \n",
    "        if (total_train_batch % validate_every == 0) or (i == len(train_dataloader) - 1):\n",
    "            val_start_time = time.time()\n",
    "            val_scores = []\n",
    "            for j, val_batch in enumerate(val_dataloader):\n",
    "                print.info(f\"    [Validation Batch {j + 1} / {len(val_dataloader)}]\", end=\"\\r\")\n",
    "                val_scores += validation_iteration(val_batch)\n",
    "                    \n",
    "            val_rouge1, val_rouge2, val_rougeL = list(zip(*val_scores))\n",
    "            \n",
    "            avg_val_rouge1 = np.mean(val_rouge1)\n",
    "            avg_val_rouge2 = np.mean(val_rouge2)\n",
    "            avg_val_rougeL = np.mean(val_rougeL)\n",
    "            \n",
    "            model_path = save_model(\n",
    "                batch=total_train_batch, \n",
    "                scores={\n",
    "                    \"rouge1\": avg_val_rouge1,\n",
    "                    \"rouge2\": avg_val_rouge2,\n",
    "                    \"rougeL\": avg_val_rougeL\n",
    "                },\n",
    "                model_dir=model_dir\n",
    "            )\n",
    "            \n",
    "            val_elapsed_time = time.time() - val_start_time\n",
    "            \n",
    "            print.info(f\"    validation : rouge1 = {avg_val_rouge1 : .4f}, rouge2 = {avg_val_rouge2 : .4f}, rougeL = {avg_val_rougeL : .4f}, time = {print_time(val_elapsed_time)} => {model_path}\")\n",
    "               \n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17c206ed-f357-473a-a1ca-894e4b3e2bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtest : rouge1 =  0.5827, rouge2 =  0.2465, rougeL =  0.5507\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "ckpt_path = \"model/memsum_sbert/gov-report/2022-12-12-11-36-03/017501.0.581-0.242-0.549.pt\"  # edit here\n",
    "load_model(ckpt_path, device=device)\n",
    "\n",
    "test_scores = []\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    print.info(f\"[Test Batch {i + 1} / {len(test_dataloader)}]\", end=\"\\r\")\n",
    "    current_test_scores = test_iteration(batch)\n",
    "\n",
    "    test_scores += current_test_scores\n",
    "\n",
    "test_rouge1, test_rouge2, test_rougeL = list(zip(*test_scores))\n",
    "            \n",
    "avg_test_rouge1 = np.mean(test_rouge1)\n",
    "avg_test_rouge2 = np.mean(test_rouge2)\n",
    "avg_test_rougeL = np.mean(test_rougeL)\n",
    "print.info(f\"test : rouge1 = {avg_test_rouge1 : .4f}, rouge2 = {avg_test_rouge2 : .4f}, rougeL = {avg_test_rougeL : .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd781343-2d36-402c-bccc-9c0c2673e586",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TOS;DR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72df1fde-43c9-41a3-9730-df5490795134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instances\n",
    "max_sentence_num = 300\n",
    "max_sequence_len = 50\n",
    "\n",
    "sbert_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "local_sentence_encoder = LocalSentenceEncoder(\n",
    "    model_name=sbert_model_name,\n",
    "    input_type=\"packed\",\n",
    "    max_sentence_num=max_sentence_num\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(sbert_model_name)\n",
    "embed_dim = local_sentence_encoder.embed_dim\n",
    "\n",
    "global_context_encoder = GlobalContextEncoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=8,\n",
    "    hidden_dim=1024,\n",
    "    num_dec_layers=3\n",
    ").to(device)\n",
    "\n",
    "extraction_context_decoder = ExtractionContextDecoder(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=8,\n",
    "    hidden_dim=1024,\n",
    "    num_dec_layers=3\n",
    ").to(device)\n",
    "\n",
    "extractor = Extractor(\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=8\n",
    ").to(device)\n",
    "\n",
    "global_context_encoder_ema = copy.deepcopy(global_context_encoder).to(device)\n",
    "extraction_context_decoder_ema = copy.deepcopy(extraction_context_decoder).to(device)\n",
    "extractor_ema = copy.deepcopy(extractor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42ba8e51-8e49-49c8-888f-a63da12637cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = \\\n",
    "[par for par in global_context_encoder.parameters() if par.requires_grad] +\\\n",
    "[par for par in extraction_context_decoder.parameters() if par.requires_grad] +\\\n",
    "[par for par in extractor.parameters() if par.requires_grad]\n",
    "\n",
    "# Memo. make lse not trainable - too small VRAM\n",
    "#[par for par in local_sentence_encoder.parameters() if par.requires_grad] +\\\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model_parameters,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=0.1,\n",
    "    patience=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01df1299-7778-4429-b1e7-b7c1b7ccf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chkpoint\n",
    "ckpt = \"model/memsum_sbert/gov-report/2022-12-12-11-36-03/017501.0.581-0.242-0.549.pt\"\n",
    "load_model(ckpt, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0b44829-3dbd-4597-871a-7bb659c6f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "class TOSDRDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        split,\n",
    "        tokenizer,\n",
    "        output_type=\"packed\",\n",
    "        mode=\"only-gold-summary\",\n",
    "        max_sequence_len=100,\n",
    "        max_sentence_num=500\n",
    "    ):\n",
    "        assert mode in [\"only-gold-summary\", \"all\"], \"Invalid param:mode\"\n",
    "        assert output_type in [\"packed\", \"padded\"], \"Invalid param:output_type\"\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.split = split\n",
    "        self.output_type = output_type\n",
    "        self.mode = mode\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.max_sentence_num = max_sentence_num\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            data_path = \"data/tos-dr/train.jsonl\"\n",
    "        elif self.split == \"val\":\n",
    "            data_path = \"data/tos-dr/val.jsonl\"\n",
    "        elif self.split == \"test\":\n",
    "            data_path = \"data/tos-dr/test.jsonl\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid param:split\")\n",
    "            \n",
    "        with open(data_path, \"r\") as f:\n",
    "            self.data = [json.loads(line) for line in f.readlines()]\n",
    "            \n",
    "        self.data = [\n",
    "            x for x in self.data \n",
    "            if not (\n",
    "                \"text\" in x and len(x[\"text\"]) == 0\n",
    "                and \"indices\" in x and len(x[\"indices\"]) == 0\n",
    "                and \"summary\" in x and len(x[\"summary\"]) == 0\n",
    "                and \"score\" in x and len(x[\"score\"]) == 0\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def is_good_sentence(self, text):\n",
    "        # bad sentence :\n",
    "        #   1) empty sentence (after strip())\n",
    "        #   2) only a single character(such as \".\") (after strip())\n",
    "        if len(text.strip()) <= 1:\n",
    "            return False\n",
    "        \n",
    "        return True  \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        doc = self.data[idx]\n",
    "        sentences = doc[\"text\"]\n",
    "        gold_summary = doc[\"summary\"]\n",
    "        \n",
    "        # select only good sentences\n",
    "        sentences = [(idx, text) for idx, text in enumerate(sentences) if self.is_good_sentence(text)]\n",
    "        sent_idx_mapper = {old_sent_idx: new_sent_idx for new_sent_idx, (old_sent_idx, _) in enumerate(sentences)}\n",
    "        sentences = [text for _, text in sentences]\n",
    "        \n",
    "        # trim sentences to max_sentence_num\n",
    "        sentences = sentences[:self.max_sentence_num]\n",
    "        sentence_num = len(sentences)\n",
    "        \n",
    "        # tokenize sentences\n",
    "        tokenized = self.tokenizer(\n",
    "            sentences,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_sequence_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = tokenized.input_ids  # [sent_num, max_seq_len]\n",
    "        attention_mask = tokenized.attention_mask  # [sent_num, max_seq_len]\n",
    "        \n",
    "        if self.output_type == \"padded\":\n",
    "            # pad input_ids\n",
    "            padded_input_ids = torch.zeros(\n",
    "                [self.max_sentence_num, self.max_sequence_len],\n",
    "                dtype=input_ids.dtype,\n",
    "                device=input_ids.device\n",
    "            )\n",
    "            padded_input_ids[:sentence_num, :] = input_ids\n",
    "            input_ids = padded_input_ids  # [max_sent_num, max_seq_len]\n",
    "            \n",
    "            # pad attention_mask\n",
    "            padded_attention_mask = torch.zeros(\n",
    "                [self.max_sentence_num, self.max_sequence_len],\n",
    "                dtype=attention_mask.dtype,\n",
    "                device=attention_mask.device\n",
    "            )\n",
    "            padded_attention_mask[:sentence_num, :] = attention_mask\n",
    "            attention_mask = padded_attention_mask  # [max_sent_num, max_seq_len]\n",
    "        \n",
    "        # make (padded) doc_mask\n",
    "        doc_mask = torch.BoolTensor([False] * sentence_num + [True] * (self.max_sentence_num - sentence_num))\n",
    "        # Note. doc_mask is not required for packed output(since no sentence needs to be masked),\n",
    "        # but just calculate it for these two reasons\n",
    "        #   - eventually packed output needs to be converted to padded output after forwarding LSE,\n",
    "        #     and doc_mask needs to be built at that moment\n",
    "        #   - to make the shape of output be same with padded output\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            oracle_summaries = doc[\"indices\"]\n",
    "            oracle_summary_scores = doc[\"score\"]\n",
    "            \n",
    "            if self.mode == \"only-gold-summary\":\n",
    "                oracle_summary = oracle_summaries[0]\n",
    "                oracle_summary_score = oracle_summary_scores[0]\n",
    "            else: # self.mode == \"all\"\n",
    "                # randomly select oracle summary\n",
    "                random_idx = np.random.choice(len(oracle_summaries))\n",
    "                oracle_summary = oracle_summaries[random_idx]\n",
    "                oracle_summary_score = oracle_summary_scores[random_idx]\n",
    "\n",
    "            # reset idxes in oracle_summary\n",
    "            oracle_summary = [sent_idx_mapper[old_sent_idx] for old_sent_idx in oracle_summary if old_sent_idx in sent_idx_mapper]\n",
    "            \n",
    "            # trim oracle_summary to max_sentence_num\n",
    "            oracle_summary = np.array(oracle_summary)\n",
    "            oracle_summary = oracle_summary[oracle_summary < sentence_num]\n",
    "\n",
    "            # shuffle oracle_summary\n",
    "            np.random.shuffle(oracle_summary)\n",
    "            oracle_summary = oracle_summary.tolist()\n",
    "\n",
    "            # pad oracle_summary (pad value = -1)\n",
    "            oracle_summary = torch.LongTensor(oracle_summary + [-1] * (self.max_sentence_num - len(oracle_summary)))\n",
    "            # Note. pad oracle_summary even for packed output,\n",
    "            # because eventually label need to be converted to padded version after forwarding LSE\n",
    "        \n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, oracle_summary, oracle_summary_score\n",
    "            \n",
    "        else: # val, test\n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary\n",
    "\n",
    "class collate_fn:\n",
    "    def __init__(self, split, output_type=\"packed\"):\n",
    "        assert split in [\"train\", \"val\", \"test\"], \"Invalid input:split\"\n",
    "        assert output_type in [\"packed\", \"padded\"], \"Invalid input:output_type\"\n",
    "        \n",
    "        self.split = split\n",
    "        self.output_type = output_type\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        if self.output_type == \"packed\":\n",
    "            input_ids = torch.cat([x[0] for x in batch], dim=0)  # tensor[sum(sent_nums), max_seq_len]\n",
    "            attention_mask = torch.cat([x[1] for x in batch], dim=0)  # tensor[sum(sent_nums), max_seq_len]\n",
    "            doc_mask = torch.stack([x[2] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            #label = torch.stack([x[3] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            sentence_num = [x[3] for x in batch]  # list[batch_size]  # same\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                oracle_summary = torch.stack([x[4] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "                oracle_summary_score = torch.tensor([x[5] for x in batch])  # tensor[batch_size]  # same\n",
    "            else: # val, test\n",
    "                sentences = [x[4] for x in batch]  # list[batch_size, sent_num]  # same\n",
    "                gold_summary = [x[5] for x in batch]  # list[batch_size, gold_summary_num]  # same\n",
    "                \n",
    "        elif self.output_type == \"padded\":\n",
    "            input_ids = torch.cat([x[0] for x in batch], dim=0)  # tensor[batch_size * max_sent_num, max_seq_len]\n",
    "            attention_mask = torch.cat([x[1] for x in batch], dim=0)  # tensor[batch_size * max_sent_num, max_seq_len]\n",
    "            doc_mask = torch.stack([x[2] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            #label = torch.stack([x[3] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "            sentence_num = [x[3] for x in batch]  # list[batch_size]  # same\n",
    "            \n",
    "            if self.split == \"train\":\n",
    "                oracle_summary = torch.stack([x[4] for x in batch], dim=0)  # tensor[batch_size, max_sent_num]  # same\n",
    "                oracle_summary_score = torch.tensor([x[5] for x in batch])  # tensor[batch_size]  # same\n",
    "            else: # val, test\n",
    "                sentences = [x[4] for x in batch]  # list[batch_size, sent_num]  # same\n",
    "                gold_summary = [x[5] for x in batch]  # list[batch_size, gold_summary_num]  # same\n",
    "        \n",
    "        if self.split == \"train\":\n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, oracle_summary, oracle_summary_score\n",
    "        \n",
    "        else: # val, test\n",
    "            return input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86da2ebd-b99d-4f30-a70f-194465b7ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset instances\n",
    "dataset_output_type = \"packed\"\n",
    "dataset_mode = \"only-gold-summary\" # [\"only-gold-summary\", \"all\"]\n",
    "batch_size = 4  # use smaller batch_size for padded_output\n",
    "num_workers = 10\n",
    "\n",
    "train_dataset = TOSDRDataset(\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    "    output_type=dataset_output_type,\n",
    "    mode=dataset_mode,\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    max_sentence_num=max_sentence_num\n",
    ")\n",
    "\n",
    "val_dataset = TOSDRDataset(\n",
    "    split=\"val\",\n",
    "    tokenizer=tokenizer,\n",
    "    output_type=dataset_output_type,\n",
    "    mode=dataset_mode,\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    max_sentence_num=max_sentence_num\n",
    ")\n",
    "\n",
    "test_dataset = TOSDRDataset(\n",
    "    split=\"test\",\n",
    "    tokenizer=tokenizer,\n",
    "    output_type=dataset_output_type,\n",
    "    mode=dataset_mode,\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    max_sentence_num=max_sentence_num\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn(\n",
    "        split=\"train\",\n",
    "        output_type=dataset_output_type\n",
    "    ),\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn(\n",
    "        split=\"val\",\n",
    "        output_type=dataset_output_type\n",
    "    ),\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn(\n",
    "        split=\"val\",\n",
    "        output_type=dataset_output_type\n",
    "    ),\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "403f95a3-f052-4f9a-bf65-f5076e8bd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probing_sentence_num = max_sentence_num\n",
    "max_probing_sentence_num = 50\n",
    "\n",
    "def train_iteration(batch):\n",
    "    input_ids, attention_mask, doc_mask, sentence_num, oracle_summary, oracle_summary_score = batch\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    doc_mask = doc_mask.to(device)  # tensor[batch_size, max_sent_num]\n",
    "    oracle_summary = oracle_summary.to(device)\n",
    "    oracle_summary_score = oracle_summary_score.to(device)\n",
    "\n",
    "    batch_size = len(sentence_num)\n",
    "\n",
    "    local_sen_embed = local_sentence_encoder(input_ids, attention_mask, sentence_num)  # [batch_size, max_sent_num, emb_dim]\n",
    "    global_context_embed = global_context_encoder(local_sen_embed, doc_mask, dropout_rate=0.1)  # [batch_size, max_sent_num, emb_dim]\n",
    "\n",
    "    doc_mask_np = doc_mask.detach().cpu().numpy()  # np[batch_size, max_sent_num]\n",
    "    remaining_mask_np = np.ones_like( doc_mask_np ).astype( bool ) | doc_mask_np\n",
    "    extraction_mask_np = np.zeros_like( doc_mask_np ).astype( bool ) | doc_mask_np\n",
    "\n",
    "    log_action_prob_list = []\n",
    "    log_stop_prob_list = []\n",
    "\n",
    "    done_list = []\n",
    "\n",
    "    extraction_context_embed = None\n",
    "\n",
    "    #for step in range(max_sentence_num):\n",
    "    for step in range(max_probing_sentence_num):\n",
    "        remaining_mask = torch.from_numpy(remaining_mask_np).to(device)  # tensor[batch_size, max_sent_num]\n",
    "        extraction_mask = torch.from_numpy(extraction_mask_np).to(device)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        if step > 0: # if at least one sentence is selected\n",
    "            extraction_context_embed = extraction_context_decoder(local_sen_embed, remaining_mask, extraction_mask, dropout_rate=0.1)\n",
    "\n",
    "        sentence_scores, p_stop, _ = extractor(local_sen_embed, global_context_embed, extraction_context_embed, extraction_mask, dropout_rate=0.1)\n",
    "        # sentence_scores : tensor[batch_size, max_sent_num]\n",
    "        # p_stop : tensor[batch_size]\n",
    "\n",
    "        p_stop = p_stop.unsqueeze(1)  # p_stop : tensor[batch_size, 1]\n",
    "        m_stop = Categorical(torch.cat([1 - p_stop, p_stop], dim=1))\n",
    "\n",
    "        # grep step-th summary sentence idx\n",
    "        summary_sent_idxs = oracle_summary[:, step]  # tensor[batch_size]\n",
    "\n",
    "        # find documents that all summary sentences are extracted(= padding is selected)\n",
    "        is_summarization_over = (summary_sent_idxs == -1)  # tensor[batch_size]\n",
    "\n",
    "        if len(done_list) > 0:\n",
    "            # is_just_stop : 이번 step에 막 summarization over 한거면 true, 아니면 false\n",
    "            is_just_stop = torch.logical_and(~done_list[-1], is_summarization_over)\n",
    "        else:\n",
    "            is_just_stop = is_summarization_over\n",
    "\n",
    "        if torch.all(is_summarization_over) and not torch.any(is_just_stop): # 모든 doc들의 summarization이 끝나고 1 step 진행한 경우 break\n",
    "            break\n",
    "\n",
    "        sentence_scores = sentence_scores.masked_fill(extraction_mask, 1e-12)\n",
    "        normalized_sentence_scores = sentence_scores / sentence_scores.sum(dim=1, keepdims=True)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        extracted_sentence_scores = normalized_sentence_scores[range(batch_size), summary_sent_idxs]  # tensor[batch_size]\n",
    "        log_action_prob = extracted_sentence_scores.masked_fill(is_summarization_over, 1.0).log()\n",
    "\n",
    "        log_stop_prob = m_stop.log_prob(is_summarization_over.long())\n",
    "        log_stop_prob = log_stop_prob.masked_fill(torch.logical_xor(is_summarization_over, is_just_stop), 0)\n",
    "\n",
    "        log_action_prob_list.append(log_action_prob.unsqueeze(1))\n",
    "        log_stop_prob_list.append(log_stop_prob.unsqueeze(1))\n",
    "        done_list.append(is_summarization_over)\n",
    "\n",
    "        for doc_idx, summary_sent_idx in enumerate(summary_sent_idxs.tolist()):\n",
    "            if summary_sent_idx != -1:\n",
    "                remaining_mask_np[doc_idx, summary_sent_idx] = False\n",
    "                extraction_mask_np[doc_idx, summary_sent_idx] = True\n",
    "\n",
    "    log_action_prob_list = torch.cat(log_action_prob_list, dim=1)\n",
    "    log_stop_prob_list = torch.cat(log_stop_prob_list, dim=1)\n",
    "    log_prob_list = log_action_prob_list + log_stop_prob_list\n",
    "\n",
    "    log_prob_list = log_prob_list.sum(dim=1) / ((log_prob_list != 0).float().sum(dim=1))\n",
    "\n",
    "    loss = (-log_prob_list * oracle_summary_score).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57b316a0-506f-43c8-8845-3637d24426d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stop_threshold = 0.6\n",
    "max_extracted_sentences_per_document = 13\n",
    "\n",
    "def validation_iteration(batch):\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary = batch\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        doc_mask = doc_mask.to(device)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        batch_size = len(sentence_num)\n",
    "\n",
    "        local_sent_embed = local_sentence_encoder(input_ids, attention_mask, sentence_num)  # [batch_size, max_sent_num, emb_dim]\n",
    "        global_context_embed = global_context_encoder_ema(local_sent_embed, doc_mask, dropout_rate=0.1)  # [batch_size, max_sent_num, emb_dim]\n",
    "        \n",
    "        doc_mask_np = doc_mask.detach().cpu().numpy()  # np[batch_size, max_sent_num]\n",
    "        \n",
    "        extracted_sents = []\n",
    "        extracted_sent_idxs = []\n",
    "        \n",
    "        sent_score_history = []\n",
    "        p_stop_history = []\n",
    "        \n",
    "        for doc_i in range(batch_size):\n",
    "            current_doc_mask_np = doc_mask_np[doc_i:doc_i + 1]\n",
    "            current_remaining_mask_np = np.ones_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            current_extraction_mask_np = np.zeros_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            \n",
    "            current_local_sent_embed = local_sent_embed[doc_i:doc_i + 1]\n",
    "            current_global_context_embed = global_context_embed[doc_i:doc_i + 1]\n",
    "            current_extraction_context_embed = None\n",
    "            \n",
    "            current_extracted_sent_idxs = []\n",
    "            extracted_sent_ngrams = set()\n",
    "            sent_score_history_for_doc_i = []\n",
    "            p_stop_history_for_doc_i = []\n",
    "            \n",
    "            for step in range(max_extracted_sentences_per_document + 1):\n",
    "                #gc.collect()\n",
    "                #torch.cuda.empty_cache()\n",
    "                \n",
    "                current_extraction_mask = torch.from_numpy(current_extraction_mask_np).to(device)\n",
    "                current_remaining_mask = torch.from_numpy(current_remaining_mask_np).to(device)\n",
    "                \n",
    "                if step > 0:\n",
    "                    current_extraction_context_embed = extraction_context_decoder_ema(\n",
    "                        current_local_sent_embed,\n",
    "                        current_remaining_mask,\n",
    "                        current_extraction_mask\n",
    "                    )\n",
    "                    \n",
    "                sent_scores, p_stop, _ = extractor_ema(\n",
    "                    current_local_sent_embed,\n",
    "                    current_global_context_embed,\n",
    "                    current_extraction_context_embed,\n",
    "                    current_extraction_mask\n",
    "                )\n",
    "                \n",
    "                p_stop = p_stop.item()\n",
    "                \n",
    "                p_stop_history_for_doc_i.append(p_stop)\n",
    "                sent_score_history_for_doc_i.append(sent_scores.detach().cpu().numpy())\n",
    "                \n",
    "                sent_scores = sent_scores.masked_fill(current_extraction_mask, 1e-12)\n",
    "                normalized_sent_scores = sent_scores / sent_scores.sum(dim=1, keepdims=True)\n",
    "                \n",
    "                is_summarization_over = (p_stop > p_stop_threshold)\n",
    "                \n",
    "                _, sorted_sent_idxs = normalized_sent_scores.sort(dim=1, descending=True)\n",
    "                sorted_sent_idxs = sorted_sent_idxs[0]\n",
    "                \n",
    "                # skip if selected sentence's ngrams are already in extracted sentences' ngram set\n",
    "                # (avoid redundancy)\n",
    "                extracted = False\n",
    "                for sent_idx in [x.item() for x in sorted_sent_idxs]:\n",
    "                    if sent_idx >= sentence_num[doc_i]:\n",
    "                        break\n",
    "                    \n",
    "                    selected_sent = sentences[doc_i][sent_idx]\n",
    "                    selected_sent_ngrams = get_ngram(selected_sent, n=3)\n",
    "                        \n",
    "                    if len(selected_sent_ngrams & extracted_sent_ngrams) < 1:\n",
    "                        extracted_sent_ngrams.update(selected_sent_ngrams)\n",
    "                        extracted = True\n",
    "                        break\n",
    "                \n",
    "                if not is_summarization_over and step < max_extracted_sentences_per_document and extracted:\n",
    "                    current_extracted_sent_idxs.append(sent_idx)\n",
    "                    current_extraction_mask_np[0, sent_idx] = True\n",
    "                    current_remaining_mask_np[0, sent_idx] = False\n",
    "                else:\n",
    "                    extracted_sents.append([sentences[doc_i][sent_idx] for sent_idx in current_extracted_sent_idxs])\n",
    "                    extracted_sent_idxs.append(current_extracted_sent_idxs)\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "            sent_score_history.append(sent_score_history_for_doc_i)\n",
    "            p_stop_history.append(p_stop_history_for_doc_i)\n",
    "        \n",
    "        scores = []\n",
    "        for doc_i in range(batch_size):\n",
    "            hyp = \"\\n\".join(extracted_sents[doc_i]).strip()\n",
    "            ref = \"\\n\".join(gold_summary[doc_i]).strip()\n",
    "\n",
    "            score = rouge_cal.score(hyp, ref)\n",
    "            scores.append((\n",
    "                score[\"rouge1\"].fmeasure,\n",
    "                score[\"rouge2\"].fmeasure,\n",
    "                score[\"rougeLsum\"].fmeasure\n",
    "            ))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9acbb67-2363-453f-b652-cfb4fd3d3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stop_threshold = 0.6\n",
    "max_extracted_sentences_per_document = 13\n",
    "\n",
    "def test_iteration(batch):\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_mask, doc_mask, sentence_num, sentences, gold_summary = batch\n",
    "        \n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        doc_mask = doc_mask.to(device)  # tensor[batch_size, max_sent_num]\n",
    "\n",
    "        batch_size = len(sentence_num)\n",
    "\n",
    "        local_sent_embed = local_sentence_encoder(input_ids, attention_mask, sentence_num)  # [batch_size, max_sent_num, emb_dim]\n",
    "        global_context_embed = global_context_encoder(local_sent_embed, doc_mask, dropout_rate=0.1)  # [batch_size, max_sent_num, emb_dim]\n",
    "        \n",
    "        doc_mask_np = doc_mask.detach().cpu().numpy()  # np[batch_size, max_sent_num]\n",
    "        \n",
    "        extracted_sents = []\n",
    "        extracted_sent_idxs = []\n",
    "        \n",
    "        sent_score_history = []\n",
    "        p_stop_history = []\n",
    "        \n",
    "        for doc_i in range(batch_size):\n",
    "            current_doc_mask_np = doc_mask_np[doc_i:doc_i + 1]\n",
    "            current_remaining_mask_np = np.ones_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            current_extraction_mask_np = np.zeros_like(current_doc_mask_np).astype(bool) | current_doc_mask_np\n",
    "            \n",
    "            current_local_sent_embed = local_sent_embed[doc_i:doc_i + 1]\n",
    "            current_global_context_embed = global_context_embed[doc_i:doc_i + 1]\n",
    "            current_extraction_context_embed = None\n",
    "            \n",
    "            current_extracted_sent_idxs = []\n",
    "            extracted_sent_ngrams = set()\n",
    "            sent_score_history_for_doc_i = []\n",
    "            p_stop_history_for_doc_i = []\n",
    "            \n",
    "            for step in range(max_extracted_sentences_per_document + 1):\n",
    "                #gc.collect()\n",
    "                #torch.cuda.empty_cache()\n",
    "                \n",
    "                current_extraction_mask = torch.from_numpy(current_extraction_mask_np).to(device)\n",
    "                current_remaining_mask = torch.from_numpy(current_remaining_mask_np).to(device)\n",
    "                \n",
    "                if step > 0:\n",
    "                    current_extraction_context_embed = extraction_context_decoder(\n",
    "                        current_local_sent_embed,\n",
    "                        current_remaining_mask,\n",
    "                        current_extraction_mask\n",
    "                    )\n",
    "                    \n",
    "                sent_scores, p_stop, _ = extractor(\n",
    "                    current_local_sent_embed,\n",
    "                    current_global_context_embed,\n",
    "                    current_extraction_context_embed,\n",
    "                    current_extraction_mask\n",
    "                )\n",
    "                \n",
    "                p_stop = p_stop.item()\n",
    "                \n",
    "                p_stop_history_for_doc_i.append(p_stop)\n",
    "                sent_score_history_for_doc_i.append(sent_scores.detach().cpu().numpy())\n",
    "                \n",
    "                sent_scores = sent_scores.masked_fill(current_extraction_mask, 1e-12)\n",
    "                normalized_sent_scores = sent_scores / sent_scores.sum(dim=1, keepdims=True)\n",
    "                \n",
    "                is_summarization_over = (p_stop > p_stop_threshold)\n",
    "                \n",
    "                _, sorted_sent_idxs = normalized_sent_scores.sort(dim=1, descending=True)\n",
    "                sorted_sent_idxs = sorted_sent_idxs[0]\n",
    "                \n",
    "                # skip if selected sentence's ngrams are already in extracted sentences' ngram set\n",
    "                # (avoid redundancy)\n",
    "                extracted = False\n",
    "                for sent_idx in [x.item() for x in sorted_sent_idxs]:\n",
    "                    if sent_idx >= sentence_num[doc_i]:\n",
    "                        break\n",
    "                    \n",
    "                    selected_sent = sentences[doc_i][sent_idx]\n",
    "                    selected_sent_ngrams = get_ngram(selected_sent, n=3)\n",
    "                        \n",
    "                    if len(selected_sent_ngrams & extracted_sent_ngrams) < 1:\n",
    "                        extracted_sent_ngrams.update(selected_sent_ngrams)\n",
    "                        extracted = True\n",
    "                        break\n",
    "                \n",
    "                if not is_summarization_over and step < max_extracted_sentences_per_document and extracted:\n",
    "                    current_extracted_sent_idxs.append(sent_idx)\n",
    "                    current_extraction_mask_np[0, sent_idx] = True\n",
    "                    current_remaining_mask_np[0, sent_idx] = False\n",
    "                else:\n",
    "                    extracted_sents.append([sentences[doc_i][sent_idx] for sent_idx in current_extracted_sent_idxs])\n",
    "                    extracted_sent_idxs.append(current_extracted_sent_idxs)\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "            sent_score_history.append(sent_score_history_for_doc_i)\n",
    "            p_stop_history.append(p_stop_history_for_doc_i)\n",
    "        \n",
    "        scores = []\n",
    "        for doc_i in range(batch_size):\n",
    "            hyp = \"\\n\".join(extracted_sents[doc_i]).strip()\n",
    "            ref = \"\\n\".join(gold_summary[doc_i]).strip()\n",
    "\n",
    "            score = rouge_cal.score(hyp, ref)\n",
    "            scores.append((\n",
    "                score[\"rouge1\"].fmeasure,\n",
    "                score[\"rouge2\"].fmeasure,\n",
    "                score[\"rougeLsum\"].fmeasure\n",
    "            ))\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5cf0ee2-32a8-4710-84a9-226dffac5ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model dir: model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53\n",
      "\n",
      "\u001b[36m[Epoch 1]\u001b[0m\n",
      "\u001b[36m  [Training Batch 100 / 403 (total_batch = 100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  4.384, elapsed time = (current) 0.973s / (mean) 1.025s\u001b[0m\n",
      "\u001b[36m  [Training Batch 200 / 403 (total_batch = 200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  4.231, elapsed time = (current) 1.418s / (mean) 0.982s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3411, rouge2 =  0.1460, rougeL =  0.3218, time = 51.429s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/000200.0.341-0.146-0.322.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 300 / 403 (total_batch = 300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  4.311, elapsed time = (current) 1.451s / (mean) 1.008s\u001b[0m\n",
      "\u001b[36m  [Training Batch 400 / 403 (total_batch = 400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  4.329, elapsed time = (current) 1.148s / (mean) 1.072s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3493, rouge2 =  0.1687, rougeL =  0.3312, time = 51.872s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/000400.0.349-0.169-0.331.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3492, rouge2 =  0.1678, rougeL =  0.3309, time = 51.296s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/000403.0.349-0.168-0.331.pt\u001b[0m\n",
      "\u001b[36m[Epoch 2]\u001b[0m\n",
      "\u001b[36m  [Training Batch 97 / 403 (total_batch = 500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.924, elapsed time = (current) 1.238s / (mean) 0.987s\u001b[0m\n",
      "\u001b[36m  [Training Batch 197 / 403 (total_batch = 600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  4.134, elapsed time = (current) 0.437s / (mean) 1.032s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3714, rouge2 =  0.1928, rougeL =  0.3534, time = 50.049s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/000600.0.371-0.193-0.353.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 297 / 403 (total_batch = 700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  4.049, elapsed time = (current) 0.650s / (mean) 1.015s\u001b[0m\n",
      "\u001b[36m  [Training Batch 397 / 403 (total_batch = 800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  4.142, elapsed time = (current) 1.036s / (mean) 1.099s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3874, rouge2 =  0.2160, rougeL =  0.3701, time = 50.635s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/000800.0.387-0.216-0.370.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3884, rouge2 =  0.2170, rougeL =  0.3711, time = 50.250s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/000806.0.388-0.217-0.371.pt\u001b[0m\n",
      "\u001b[36m[Epoch 3]\u001b[0m\n",
      "\u001b[36m  [Training Batch 94 / 403 (total_batch = 900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.571, elapsed time = (current) 1.143s / (mean) 1.015s\u001b[0m\n",
      "\u001b[36m  [Training Batch 194 / 403 (total_batch = 1000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.886, elapsed time = (current) 0.639s / (mean) 1.037s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3948, rouge2 =  0.2288, rougeL =  0.3785, time = 51.283s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001000.0.395-0.229-0.378.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 294 / 403 (total_batch = 1100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.918, elapsed time = (current) 0.885s / (mean) 1.039s\u001b[0m\n",
      "\u001b[36m  [Training Batch 394 / 403 (total_batch = 1200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.882, elapsed time = (current) 1.396s / (mean) 1.010s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.4006, rouge2 =  0.2397, rougeL =  0.3854, time = 50.620s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001200.0.401-0.240-0.385.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.4008, rouge2 =  0.2396, rougeL =  0.3856, time = 51.071s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001209.0.401-0.240-0.386.pt\u001b[0m\n",
      "\u001b[36m[Epoch 4]\u001b[0m\n",
      "\u001b[36m  [Training Batch 91 / 403 (total_batch = 1300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.206, elapsed time = (current) 0.691s / (mean) 0.897s\u001b[0m\n",
      "\u001b[36m  [Training Batch 191 / 403 (total_batch = 1400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.666, elapsed time = (current) 0.530s / (mean) 1.083s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.4037, rouge2 =  0.2467, rougeL =  0.3889, time = 52.211s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001400.0.404-0.247-0.389.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 291 / 403 (total_batch = 1500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.603, elapsed time = (current) 0.779s / (mean) 1.078s\u001b[0m\n",
      "\u001b[36m  [Training Batch 391 / 403 (total_batch = 1600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.721, elapsed time = (current) 1.756s / (mean) 1.083s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.4013, rouge2 =  0.2415, rougeL =  0.3857, time = 51.484s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001600.0.401-0.242-0.386.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.4011, rouge2 =  0.2414, rougeL =  0.3855, time = 51.581s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001612.0.401-0.241-0.386.pt\u001b[0m\n",
      "\u001b[36m[Epoch 5]\u001b[0m\n",
      "\u001b[36m  [Training Batch 88 / 403 (total_batch = 1700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.857, elapsed time = (current) 1.167s / (mean) 0.893s\u001b[0m\n",
      "\u001b[36m  [Training Batch 188 / 403 (total_batch = 1800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.398, elapsed time = (current) 0.804s / (mean) 1.054s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.4015, rouge2 =  0.2429, rougeL =  0.3861, time = 53.190s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001800.0.402-0.243-0.386.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 288 / 403 (total_batch = 1900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.392, elapsed time = (current) 1.469s / (mean) 1.045s\u001b[0m\n",
      "\u001b[36m  [Training Batch 388 / 403 (total_batch = 2000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.360, elapsed time = (current) 0.745s / (mean) 1.001s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.4010, rouge2 =  0.2457, rougeL =  0.3859, time = 51.633s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002000.0.401-0.246-0.386.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3999, rouge2 =  0.2435, rougeL =  0.3846, time = 50.814s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002015.0.400-0.244-0.385.pt\u001b[0m\n",
      "\u001b[36m[Epoch 6]\u001b[0m\n",
      "\u001b[36m  [Training Batch 85 / 403 (total_batch = 2100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.584, elapsed time = (current) 0.534s / (mean) 0.909s\u001b[0m\n",
      "\u001b[36m  [Training Batch 185 / 403 (total_batch = 2200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.063, elapsed time = (current) 0.775s / (mean) 1.086s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3832, rouge2 =  0.2383, rougeL =  0.3695, time = 49.450s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002200.0.383-0.238-0.369.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 285 / 403 (total_batch = 2300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.141, elapsed time = (current) 1.538s / (mean) 1.034s\u001b[0m\n",
      "\u001b[36m  [Training Batch 385 / 403 (total_batch = 2400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  3.136, elapsed time = (current) 1.060s / (mean) 1.037s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3562, rouge2 =  0.2194, rougeL =  0.3430, time = 46.760s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002400.0.356-0.219-0.343.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3533, rouge2 =  0.2178, rougeL =  0.3404, time = 46.187s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002418.0.353-0.218-0.340.pt\u001b[0m\n",
      "\u001b[36m[Epoch 7]\u001b[0m\n",
      "\u001b[36m  [Training Batch 82 / 403 (total_batch = 2500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.255, elapsed time = (current) 1.384s / (mean) 0.828s\u001b[0m\n",
      "\u001b[36m  [Training Batch 182 / 403 (total_batch = 2600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.875, elapsed time = (current) 0.445s / (mean) 1.014s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3245, rouge2 =  0.1983, rougeL =  0.3117, time = 42.565s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002600.0.324-0.198-0.312.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 282 / 403 (total_batch = 2700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.993, elapsed time = (current) 0.651s / (mean) 1.032s\u001b[0m\n",
      "\u001b[36m  [Training Batch 382 / 403 (total_batch = 2800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.883, elapsed time = (current) 1.206s / (mean) 1.009s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3189, rouge2 =  0.1973, rougeL =  0.3068, time = 41.218s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002800.0.319-0.197-0.307.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.3132, rouge2 =  0.1946, rougeL =  0.3013, time = 40.862s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/002821.0.313-0.195-0.301.pt\u001b[0m\n",
      "\u001b[36m[Epoch 8]\u001b[0m\n",
      "\u001b[36m  [Training Batch 79 / 403 (total_batch = 2900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.155, elapsed time = (current) 1.282s / (mean) 0.846s\u001b[0m\n",
      "\u001b[36m  [Training Batch 179 / 403 (total_batch = 3000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.582, elapsed time = (current) 1.144s / (mean) 1.015s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2915, rouge2 =  0.1805, rougeL =  0.2798, time = 40.084s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/003000.0.291-0.181-0.280.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 279 / 403 (total_batch = 3100)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.735, elapsed time = (current) 1.326s / (mean) 1.003s\u001b[0m\n",
      "\u001b[36m  [Training Batch 379 / 403 (total_batch = 3200)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.816, elapsed time = (current) 0.814s / (mean) 1.027s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2738, rouge2 =  0.1636, rougeL =  0.2619, time = 39.289s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/003200.0.274-0.164-0.262.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2717, rouge2 =  0.1628, rougeL =  0.2600, time = 38.585s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/003224.0.272-0.163-0.260.pt\u001b[0m\n",
      "\u001b[36m[Epoch 9]\u001b[0m\n",
      "\u001b[36m  [Training Batch 76 / 403 (total_batch = 3300)]\u001b[0m\n",
      "\u001b[36m    training : loss =  1.974, elapsed time = (current) 0.773s / (mean) 0.776s\u001b[0m\n",
      "\u001b[36m  [Training Batch 176 / 403 (total_batch = 3400)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.519, elapsed time = (current) 1.113s / (mean) 1.056s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2557, rouge2 =  0.1553, rougeL =  0.2439, time = 36.838s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/003400.0.256-0.155-0.244.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 276 / 403 (total_batch = 3500)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.440, elapsed time = (current) 0.629s / (mean) 0.991s\u001b[0m\n",
      "\u001b[36m  [Training Batch 376 / 403 (total_batch = 3600)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.669, elapsed time = (current) 1.016s / (mean) 1.043s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2546, rouge2 =  0.1533, rougeL =  0.2419, time = 35.974s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/003600.0.255-0.153-0.242.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2553, rouge2 =  0.1550, rougeL =  0.2428, time = 35.907s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/003627.0.255-0.155-0.243.pt\u001b[0m\n",
      "\u001b[36m[Epoch 10]\u001b[0m\n",
      "\u001b[36m  [Training Batch 73 / 403 (total_batch = 3700)]\u001b[0m\n",
      "\u001b[36m    training : loss =  1.830, elapsed time = (current) 1.138s / (mean) 0.779s\u001b[0m\n",
      "\u001b[36m  [Training Batch 173 / 403 (total_batch = 3800)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.309, elapsed time = (current) 1.585s / (mean) 1.013s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2494, rouge2 =  0.1472, rougeL =  0.2357, time = 35.880s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/003800.0.249-0.147-0.236.pt\u001b[0m\n",
      "\u001b[36m  [Training Batch 273 / 403 (total_batch = 3900)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.458, elapsed time = (current) 1.320s / (mean) 1.051s\u001b[0m\n",
      "\u001b[36m  [Training Batch 373 / 403 (total_batch = 4000)]\u001b[0m\n",
      "\u001b[36m    training : loss =  2.539, elapsed time = (current) 0.851s / (mean) 1.058s\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2511, rouge2 =  0.1448, rougeL =  0.2358, time = 35.899s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/004000.0.251-0.145-0.236.pt\u001b[0m\n",
      "\u001b[36m    validation : rouge1 =  0.2527, rouge2 =  0.1451, rougeL =  0.2372, time = 36.269s => model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/004030.0.253-0.145-0.237.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "cur_time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "#model_dir = os.path.join(\"model/memsum_sbert/tos-dr\", dataset_mode, cur_time)\n",
    "model_dir = os.path.join(\"model/memsum_sbert/tos-dr-from-pretrained\", dataset_mode, cur_time)\n",
    "\n",
    "epochs = 10  # 50\n",
    "print_every = 100  # 100\n",
    "validate_every = 200  # 100\n",
    "early_stop = False  # False\n",
    "\n",
    "print(f\"model dir: {model_dir}\")\n",
    "print()\n",
    "\n",
    "total_train_batch = 0\n",
    "for epoch in range(epochs):\n",
    "    print.info(f\"[Epoch {epoch + 1}]\")\n",
    "    running_loss = 0\n",
    "    total_train_elapsed_time = 0\n",
    "    \n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        train_start_time = time.time()\n",
    "        total_train_batch += 1\n",
    "        \n",
    "        loss = train_iteration(train_batch)\n",
    "        running_loss += loss\n",
    "        \n",
    "        #scheduler.step()\n",
    "        \n",
    "        update_moving_average(global_context_encoder_ema, global_context_encoder)\n",
    "        update_moving_average(extraction_context_decoder_ema, extraction_context_decoder)\n",
    "        update_moving_average(extractor_ema, extractor)\n",
    "        \n",
    "        train_elapsed_time = time.time() - train_start_time\n",
    "        total_train_elapsed_time += train_elapsed_time\n",
    "        \n",
    "        if total_train_batch % print_every == 0:\n",
    "            print.info(f\"  [Training Batch {i + 1} / {len(train_dataloader)} (total_batch = {total_train_batch})]\")\n",
    "            #current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            #(tensor_num, tensor_size), (param_num, param_size) = analyzeGPU()\n",
    "            \n",
    "            print.info(f\"    training : loss = {running_loss / print_every : .3f}, elapsed time = (current) {print_time(train_elapsed_time)} / (mean) {print_time(total_train_elapsed_time / print_every)}\")\n",
    "            #print.info(f\"    actual step = {actual_step}, # of cuda tensors = {tensor_num}, mem = {tensor_size}\")\n",
    "            running_loss = 0\n",
    "            total_train_elapsed_time = 0\n",
    "                \n",
    "        if (total_train_batch % validate_every == 0) or (i == len(train_dataloader) - 1):\n",
    "            val_start_time = time.time()\n",
    "            val_scores = []\n",
    "            for j, val_batch in enumerate(val_dataloader):\n",
    "                print.info(f\"    [Validation Batch {j + 1} / {len(val_dataloader)}]\", end=\"\\r\")\n",
    "                val_scores += validation_iteration(val_batch)\n",
    "                    \n",
    "            val_rouge1, val_rouge2, val_rougeL = list(zip(*val_scores))\n",
    "            \n",
    "            avg_val_rouge1 = np.mean(val_rouge1)\n",
    "            avg_val_rouge2 = np.mean(val_rouge2)\n",
    "            avg_val_rougeL = np.mean(val_rougeL)\n",
    "            \n",
    "            model_path = save_model(\n",
    "                batch=total_train_batch, \n",
    "                scores={\n",
    "                    \"rouge1\": avg_val_rouge1,\n",
    "                    \"rouge2\": avg_val_rouge2,\n",
    "                    \"rougeL\": avg_val_rougeL\n",
    "                },\n",
    "                model_dir=model_dir\n",
    "            )\n",
    "            \n",
    "            val_elapsed_time = time.time() - val_start_time\n",
    "            \n",
    "            print.info(f\"    validation : rouge1 = {avg_val_rouge1 : .4f}, rouge2 = {avg_val_rouge2 : .4f}, rougeL = {avg_val_rougeL : .4f}, time = {print_time(val_elapsed_time)} => {model_path}\")\n",
    "               \n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6002eff8-abbd-413d-9001-8e2675b8a5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mtest : rouge1 =  0.4178, rouge2 =  0.2631, rougeL =  0.4034\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "#ckpt_path = \"model/memsum_sbert/tos-dr/only-gold-summary/2022-12-15-03-43-15/001612.0.419-0.267-0.406.pt\"\n",
    "#ckpt_path = \"model/memsum_sbert/tos-dr/all/2022-12-15-06-21-02/002000.0.421-0.272-0.407.pt\"\n",
    "ckpt_path = \"model/memsum_sbert/tos-dr-from-pretrained/only-gold-summary/2022-12-15-08-33-53/001600.0.401-0.242-0.386.pt\"\n",
    "load_model(ckpt_path, device=device)\n",
    "\n",
    "test_scores = []\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    print.info(f\"[Test Batch {i + 1} / {len(test_dataloader)}]\", end=\"\\r\")\n",
    "    current_test_scores = test_iteration(batch)\n",
    "\n",
    "    test_scores += current_test_scores\n",
    "\n",
    "test_rouge1, test_rouge2, test_rougeL = list(zip(*test_scores))\n",
    "\n",
    "avg_test_rouge1 = np.mean(test_rouge1)\n",
    "avg_test_rouge2 = np.mean(test_rouge2)\n",
    "avg_test_rougeL = np.mean(test_rougeL)\n",
    "print.info(f\"test : rouge1 = {avg_test_rouge1 : .4f}, rouge2 = {avg_test_rouge2 : .4f}, rougeL = {avg_test_rougeL : .4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126ede1-0410-42d5-bf45-7631a6218c9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ba1045-19ef-493c-9aec-59c6e3660b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = iter(train_dataloader).__next__()\n",
    "input_ids, attention_mask, doc_mask, label, sentence_num, oracle_summary, oracle_summary_score = batch\n",
    "\n",
    "input_ids, attention_mask, doc_mask = input_ids.to(device), attention_mask.to(device), doc_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990da102-3011-4b5c-8c64-02c29eb0c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_sen_embed = local_sentence_encoder(input_ids, attention_mask, sentence_num)  # [batch_size, max_sent_num, emb_dim]\n",
    "global_context_embed = global_context_encoder(local_sen_embed, doc_mask, dropout_rate=0.1)  # [batch_size, max_sent_num, emb_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39cab1ce-c4dc-4d19-9fc0-463ebe879f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_mask_np = doc_mask.detach().cpu().numpy()\n",
    "remaining_mask_np = np.ones_like( doc_mask_np ).astype( bool ) | doc_mask_np\n",
    "extraction_mask_np = np.zeros_like( doc_mask_np ).astype( bool ) | doc_mask_np\n",
    "\n",
    "# True : masked(not visible), False : not masked(visible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b7dba5e-d347-4b2f-8c67-12fb3f7831b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_action_prob_list = []\n",
    "log_stop_prob_list = []\n",
    "\n",
    "done_list = []\n",
    "extraction_context_embed = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baa8ef1c-2263-458a-b934-e572978dcc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_sen_idxs.shape[1] == max_sentence_num\n",
    "for step in range(max_sentence_num):\n",
    "    remaining_mask = torch.from_numpy(remaining_mask_np).to(device)\n",
    "    extraction_mask = torch.from_numpy(extraction_mask_np).to(device)\n",
    "    \n",
    "    if step > 0: # if at least one sentence is selected\n",
    "        extraction_context_embed = extraction_context_decoder(local_sen_embed, remaining_mask, extraction_mask, dropout_rate=0.1)\n",
    "    \n",
    "    sentence_scores, p_stop, baseline = extractor(local_sen_embed, global_context_embed, extraction_context_embed, extraction_mask, dropout_rate=0.1)\n",
    "    # p : tensor[batch_size, max_sent_num]\n",
    "    # Note. baseline is not used -> deletable?\n",
    "    \n",
    "    p_stop = p_stop.unsqueeze(1)\n",
    "    m_stop = Categorical(torch.cat([1 - p_stop, p_stop], dim=1))\n",
    "    \n",
    "    # grep step-th summary sentence idx\n",
    "    summary_sent_idxs = oracle_summary[:, step]  # tensor[batch_size]\n",
    "    \n",
    "    # find documents that all summary sentences are extracted(= padding is selected)\n",
    "    is_summarization_over = (summary_sent_idxs == -1)  # tensor[batch_size]\n",
    "    \n",
    "    if len(done_list) > 0:\n",
    "        # is_just_stop : 이번 step에 막 summarization over 한거면 true, 아니면 false\n",
    "        is_just_stop = torch.logical_and(~done_list[-1], is_summarization_over)\n",
    "    else:\n",
    "        is_just_stop = is_summarization_over\n",
    "    \n",
    "    if torch.all(is_summarization_over) and not torch.any(just_stop): # 모든 doc들의 summarization이 끝나고 1 step 진행한 경우 break\n",
    "        break\n",
    "    \n",
    "    sentence_scores = sentence_scores.masked_fill(extraction_mask, 1e-12)\n",
    "    normalized_sentence_scores = sentence_scores / sentence_scores.sum(dim=1, keepdims=True)  # tensor[batch_size, max_sent_num]\n",
    "    \n",
    "    extracted_sentence_scores = normalized_sentence_scores[range(batch_size), summary_sent_idxs]  # tensor[batch_size]\n",
    "    log_action_prob = extracted_sentence_scores.masked_fill(is_summarization_over, 1.0).log()\n",
    "    \n",
    "    log_stop_prob = m_stop.log_prob(is_summarization_over.to(torch.long))\n",
    "    log_stop_prob = log_stop_prob.masked_fill(torch.logical_xor(is_summarization_over, is_just_stop), 0)\n",
    "    \n",
    "    log_action_prob_list.append(log_action_prob.unsqueeze(1))\n",
    "    log_stop_prob_list.append(log_stop_prob.unsqueeze(1))\n",
    "    done_list.append(is_summarization_over)\n",
    "    \n",
    "    for doc_idx, summary_sent_idx in enumerate(summary_sent_idxs.tolist()):\n",
    "        if summary_sent_idx != -1:\n",
    "            remaining_mask_np[doc_idx, summary_sent_idx] = False\n",
    "            extraction_mask_np[doc_idx, summary_sent_idx] = True\n",
    "    log_action\n",
    "        \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cecf12b-d74d-4a37-8a1f-391fe036e41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69, 153, 0, 48, 78, 56, 82, 119, 10, 67, 223, 225, 16, 5, 11, 28]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_idxs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "797c1163-73a9-4b8e-9714-b9b30764c792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0051, 0.0039, 0.0227, 0.0069, 0.0083, 0.0086, 0.0020, 0.0027, 0.0050,\n",
       "        0.0025, 0.0040, 0.0036, 0.0041, 0.0051, 0.0096, 0.0021],\n",
       "       device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "917c35bd-9016-440c-a023-cf0009b771f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ...,  True,  True,  True],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [False, False, False, ...,  True,  True,  True],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_mask_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1bb92fa-9b50-4ec6-86bd-153468f3a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_action_prob_list = []\n",
    "log_stop_prob_list = []\n",
    "\n",
    "done_list = []\n",
    "extraction_context_embed = None\n",
    "\n",
    "for step in range(valid_sen_idxs.shape[1]):\n",
    "    remaining_mask = torch.from_numpy( remaining_mask_np ).to(device)\n",
    "    extraction_mask = torch.from_numpy( extraction_mask_np ).to(device)\n",
    "    if step > 0:\n",
    "        extraction_context_embed = extraction_context_decoder( local_sen_embed, remaining_mask, extraction_mask, dropout_rate )\n",
    "    p, p_stop, baseline = extractor( local_sen_embed, global_context_embed, extraction_context_embed , extraction_mask , dropout_rate )\n",
    "\n",
    "    p_stop = p_stop.unsqueeze(1)\n",
    "    m_stop = Categorical( torch.cat( [ 1-p_stop, p_stop  ], dim =1 ) )\n",
    "\n",
    "    sen_indices = valid_sen_idxs[:, step]\n",
    "    done = sen_indices == -1\n",
    "    if len(done_list) > 0:\n",
    "        done = torch.logical_or(done_list[-1], done)\n",
    "        just_stop = torch.logical_and( ~done_list[-1], done )\n",
    "    else:\n",
    "        just_stop = done\n",
    "\n",
    "    if torch.all( done ) and not torch.any(just_stop):\n",
    "        break\n",
    "\n",
    "    p = p.masked_fill( extraction_mask, 1e-12 )  \n",
    "    normalized_p = p / p.sum(dim=1, keepdims = True)\n",
    "    ## Here the sen_indices is actually pre-sampled action\n",
    "    normalized_p = normalized_p[ np.arange( num_documents ), sen_indices ]\n",
    "    log_action_prob = normalized_p.masked_fill( done, 1.0 ).log()\n",
    "\n",
    "    log_stop_prob = m_stop.log_prob( done.to(torch.long)  )\n",
    "    log_stop_prob = log_stop_prob.masked_fill( torch.logical_xor( done, just_stop ), 0.0 )\n",
    "\n",
    "    log_action_prob_list.append( log_action_prob.unsqueeze(1) )\n",
    "    log_stop_prob_list.append( log_stop_prob.unsqueeze(1) )\n",
    "    done_list.append(done)\n",
    "\n",
    "    for doc_i in range( num_documents ):\n",
    "        sen_i = sen_indices[ doc_i ].item()\n",
    "        if sen_i != -1:\n",
    "            remaining_mask_np[doc_i,sen_i] = False\n",
    "            extraction_mask_np[doc_i,sen_i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830365d5-88d0-4aa9-bd19-90855df708c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3863, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf14697-70dd-408a-a675-6068579a513d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6684386-8851-4b90-bc15-238902d902ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mean_pooling(lse_output, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3da7c8cb-0a78-426c-a7fa-809bdfe6d98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([440, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204e7ed8-1599-4abd-b97c-54c6ac7e440d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([440, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lse_output.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e31514b-eba6-44ee-8d9f-f0ba5f0d1d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 768)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f62f4-a8bc-4ec7-ab94-3696015d10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teration(batch):\n",
    "    sentences, label = batch\n",
    "    label = label.to(device)\n",
    "    \n",
    "    batch_size = sentences.shape[0]\n",
    "    sentence_num = sentences.shape[1]\n",
    "    \n",
    "    local_sentence_embedding = local_sentence_encoder.encode(\n",
    "        batch[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df2092a4-c6cb-44e6-b764-cd9410d722d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "834d4dec-f6b0-4e98-8175-2c893ad751dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12cf69d3-ebbb-4ff3-8440-56e3f2b1763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExtractionTrainingDataset(\n",
    "    \"data/gov-report/train_GOVREPORT.jsonl\",\n",
    "    tokenizer=model.tokenizer,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0851ce73-638a-42d8-ae08-c8a5c779b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "62280d36-7e7c-4625-8dfb-dde35596b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iter(dataloader).__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "398dd4f7-c8ce-487a-af68-c05a8d21c3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37be63d-a40d-443e-923f-e8918fe571b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemSum:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads=8,\n",
    "        hidden_dim=1024,\n",
    "        N_enc_l=2 ,\n",
    "        N_enc_g=2,\n",
    "        N_dec=3,\n",
    "        max_seq_len=100,\n",
    "        max_doc_len=500\n",
    "    ):\n",
    "        self.device = \"cuda:0\"\n",
    "        \n",
    "        self.local_sentence_encoder = SentenceTransformer(\"all-mpnet-base-v2\").to(self.device)\n",
    "        embed_dim = self.local_sentence_encoder.get_sentence_embedding_dimension()\n",
    "        \n",
    "        self.global_context_encoder = GlobalContextEncoder( embed_dim, num_heads, hidden_dim, N_enc_g ).to(self.device)\n",
    "        \n",
    "        self.extraction_context_decoder = GlobalContextEncoder( embed_dim, num_heads, hidden_dim, N_dec ).to(self.device)\n",
    "        \n",
    "        self.extractor = Extractor( embed_dim, num_heads ).to(self.device)\n",
    "        \n",
    "        \"\"\"\n",
    "        ckpt = torch.load( model_path, map_location = \"cpu\" )\n",
    "        self.local_sentence_encoder.load_state_dict( ckpt[\"local_sentence_encoder\"] )\n",
    "        self.global_context_encoder.load_state_dict( ckpt[\"global_context_encoder\"] )\n",
    "        self.extraction_context_decoder.load_state_dict( ckpt[\"extraction_context_decoder\"] )\n",
    "        self.extractor.load_state_dict(ckpt[\"extractor\"])\n",
    "        \"\"\"\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "    \n",
    "    def extract( self, document_batch, p_stop_thres = 0.7, ngram_blocking = False, ngram = 3, return_sentence_position = False, return_sentence_score_history = False, max_extracted_sentences_per_document = 4 ):\n",
    "        \"\"\"document_batch is a batch of documents:\n",
    "        [  [ sen1, sen2, ... , senL1 ], \n",
    "           [ sen1, sen2, ... , senL2], ...\n",
    "         ]\n",
    "        \"\"\"\n",
    "        extracted_sentences = []\n",
    "        sentence_score_history = []\n",
    "        p_stop_history = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            num_sentences = seqs.size(1)\n",
    "            sen_embed  = self.local_sentence_encoder( seqs.view(-1, seqs.size(2) )  )\n",
    "            sen_embed = sen_embed.view( -1, num_sentences, sen_embed.size(1) )\n",
    "            relevance_embed = self.global_context_encoder( sen_embed, doc_mask  )\n",
    "    \n",
    "            num_documents = seqs.size(0)\n",
    "            doc_mask = doc_mask.detach().cpu().numpy()\n",
    "            seqs = seqs.detach().cpu().numpy()\n",
    "    \n",
    "            extracted_sentences = []\n",
    "            extracted_sentences_positions = []\n",
    "        \n",
    "            for doc_i in range(num_documents):\n",
    "                current_doc_mask = doc_mask[doc_i:doc_i+1]\n",
    "                current_remaining_mask_np = np.ones_like(current_doc_mask ).astype(np.bool) | current_doc_mask\n",
    "                current_extraction_mask_np = np.zeros_like(current_doc_mask).astype(np.bool) | current_doc_mask\n",
    "        \n",
    "                current_sen_embed = sen_embed[doc_i:doc_i+1]\n",
    "                current_relevance_embed = relevance_embed[ doc_i:doc_i+1 ]\n",
    "                current_redundancy_embed = None\n",
    "        \n",
    "                current_hyps = []\n",
    "                extracted_sen_ngrams = set()\n",
    "\n",
    "                sentence_score_history_for_doc_i = []\n",
    "\n",
    "                p_stop_history_for_doc_i = []\n",
    "                \n",
    "                for step in range( max_extracted_sentences_per_document+1 ) :\n",
    "                    current_extraction_mask = torch.from_numpy( current_extraction_mask_np ).to(self.device)\n",
    "                    current_remaining_mask = torch.from_numpy( current_remaining_mask_np ).to(self.device)\n",
    "                    if step > 0:\n",
    "                        current_redundancy_embed = self.extraction_context_decoder( current_sen_embed, current_remaining_mask, current_extraction_mask  )\n",
    "                    p, p_stop, _ = self.extractor( current_sen_embed, current_relevance_embed, current_redundancy_embed , current_extraction_mask  )\n",
    "                    p_stop = p_stop.unsqueeze(1)\n",
    "            \n",
    "            \n",
    "                    p = p.masked_fill( current_extraction_mask, 1e-12 ) \n",
    "\n",
    "                    sentence_score_history_for_doc_i.append( p.detach().cpu().numpy() )\n",
    "\n",
    "                    p_stop_history_for_doc_i.append(  p_stop.squeeze(1).item() )\n",
    "\n",
    "                    normalized_p = p / p.sum(dim=1, keepdims = True)\n",
    "\n",
    "                    stop = p_stop.squeeze(1).item()> p_stop_thres #and step > 0\n",
    "                    \n",
    "                    #sen_i = normalized_p.argmax(dim=1)[0]\n",
    "                    _, sorted_sen_indices =normalized_p.sort(dim=1, descending= True)\n",
    "                    sorted_sen_indices = sorted_sen_indices[0]\n",
    "                    \n",
    "                    extracted = False\n",
    "                    for sen_i in sorted_sen_indices:\n",
    "                        sen_i = sen_i.item()\n",
    "                        if sen_i< len(document_batch[doc_i]):\n",
    "                            sen = document_batch[doc_i][sen_i]\n",
    "                        else:\n",
    "                            break\n",
    "                        sen_ngrams = self.get_ngram( sen.lower().split(), ngram )\n",
    "                        if not ngram_blocking or len( extracted_sen_ngrams &  sen_ngrams ) < 1:\n",
    "                            extracted_sen_ngrams.update( sen_ngrams )\n",
    "                            extracted = True\n",
    "                            break\n",
    "                                        \n",
    "                    if stop or step == max_extracted_sentences_per_document or not extracted:\n",
    "                        extracted_sentences.append( [ document_batch[doc_i][sen_i] for sen_i in  current_hyps if sen_i < len(document_batch[doc_i])    ] )\n",
    "                        extracted_sentences_positions.append( [ sen_i for sen_i in  current_hyps if sen_i < len(document_batch[doc_i])  ]  )\n",
    "                        break\n",
    "                    else:\n",
    "                        current_hyps.append(sen_i)\n",
    "                        current_extraction_mask_np[0, sen_i] = True\n",
    "                        current_remaining_mask_np[0, sen_i] = False\n",
    "\n",
    "                sentence_score_history.append(sentence_score_history_for_doc_i)\n",
    "                p_stop_history.append( p_stop_history_for_doc_i )\n",
    "\n",
    "        # if return_sentence_position:\n",
    "        #     return extracted_sentences, extracted_sentences_positions \n",
    "        # else:\n",
    "        #     return extracted_sentences\n",
    "\n",
    "        results = [extracted_sentences]\n",
    "        if return_sentence_position:\n",
    "            results.append( extracted_sentences_positions )\n",
    "        if return_sentence_score_history:\n",
    "            results+=[sentence_score_history , p_stop_history ]\n",
    "        if len(results) == 1:\n",
    "            results = results[0]\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "class ExtractiveSummarizer_NeuSum:\n",
    "    def __init__( self, model_path, vocabulary_path, gpu = None , embed_dim=200,\n",
    "                 max_seq_len =100, max_doc_len = 500 , **kwargs ):\n",
    "        with open( vocabulary_path , \"rb\" ) as f:\n",
    "            words = pickle.load(f)\n",
    "        self.vocab = Vocab_NeuSum( words )\n",
    "        vocab_size = len(words)\n",
    "        self.local_sentence_encoder = LocalSentenceEncoder_NeuSum( vocab_size, self.vocab.pad_index, embed_dim, None )\n",
    "        self.global_context_encoder = GlobalContextEncoder_NeuSum( embed_dim)\n",
    "        self.extraction_context_decoder = ExtractionContextDecoder_NeuSum( embed_dim)\n",
    "        self.extractor = Extractor_NeuSum( embed_dim )\n",
    "        ckpt = torch.load( model_path, map_location = \"cpu\" )\n",
    "        self.local_sentence_encoder.load_state_dict( ckpt[\"local_sentence_encoder\"] )\n",
    "        self.global_context_encoder.load_state_dict( ckpt[\"global_context_encoder\"] )\n",
    "        self.extraction_context_decoder.load_state_dict( ckpt[\"extraction_context_decoder\"] )\n",
    "        self.extractor.load_state_dict(ckpt[\"extractor\"])\n",
    "        \n",
    "        self.device =  torch.device( \"cuda:%d\"%(gpu) if gpu is not None and torch.cuda.is_available() else \"cpu\"  )        \n",
    "        self.local_sentence_encoder.to(self.device)\n",
    "        self.global_context_encoder.to(self.device)\n",
    "        self.extraction_context_decoder.to(self.device)\n",
    "        self.extractor.to(self.device)\n",
    "        \n",
    "        self.sentence_tokenizer = SentenceTokenizer_NeuSum()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.max_doc_len = max_doc_len\n",
    "    \n",
    "\n",
    "    def extract( self, document_batch, return_sentence_position = False, max_extracted_sentences_per_document = 7, **kwargs ):\n",
    "        \"\"\"document_batch is a batch of documents:\n",
    "        [  [ sen1, sen2, ... , senL1 ], \n",
    "           [ sen1, sen2, ... , senL2], ...\n",
    "         ]\n",
    "        \"\"\"\n",
    "        ## tokenization:\n",
    "        document_length_list = []\n",
    "        sentence_length_list = []\n",
    "        tokenized_document_batch = []\n",
    "        for document in document_batch:\n",
    "            tokenized_document = []\n",
    "            for sen in document:\n",
    "                tokenized_sen = self.sentence_tokenizer.tokenize( sen )\n",
    "                tokenized_document.append( tokenized_sen )\n",
    "                sentence_length_list.append( len(tokenized_sen.split()) )\n",
    "            tokenized_document_batch.append( tokenized_document )\n",
    "            document_length_list.append( len(tokenized_document) )\n",
    "\n",
    "        max_document_length =  self.max_doc_len \n",
    "        max_sentence_length =  self.max_seq_len \n",
    "        ## convert to sequence\n",
    "        seqs = []\n",
    "        doc_mask = []\n",
    "        \n",
    "        for document in tokenized_document_batch:\n",
    "            if len(document) > max_document_length:\n",
    "                # doc_mask.append(  [0] * max_document_length )\n",
    "                document = document[:max_document_length]\n",
    "            else:\n",
    "                # doc_mask.append(  [0] * len(document) +[1] * ( max_document_length -  len(document) ) )\n",
    "                document = document + [\"\"] * ( max_document_length -  len(document) )\n",
    "\n",
    "            doc_mask.append(  [ 1 if sen.strip() == \"\" else 0 for sen in  document   ] )\n",
    "\n",
    "            document_sequences = []\n",
    "            for sen in document:\n",
    "                seq = self.vocab.sent2seq( sen, max_sentence_length )\n",
    "                document_sequences.append(seq)\n",
    "            seqs.append(document_sequences)\n",
    "        seqs = np.asarray(seqs)\n",
    "        doc_mask = np.asarray(doc_mask) == 1\n",
    "        seqs = torch.from_numpy(seqs).to(self.device)\n",
    "        doc_mask = torch.from_numpy(doc_mask).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            num_sentences = seqs.size(1)\n",
    "            sen_embed  = self.local_sentence_encoder( seqs.view(-1, seqs.size(2) )  )\n",
    "            sen_embed = sen_embed.view( -1, num_sentences, sen_embed.size(1) )\n",
    "            global_context_embed, backward_state = self.global_context_encoder( sen_embed, doc_mask,  return_backward_state = True  )\n",
    "    \n",
    "            num_documents = seqs.size(0)\n",
    "            doc_mask = doc_mask.detach().cpu().numpy()\n",
    "            seqs = seqs.detach().cpu().numpy()\n",
    "    \n",
    "            extracted_sentences = []\n",
    "            extracted_sentences_positions = []\n",
    "        \n",
    "            for doc_i in range(num_documents):\n",
    "                current_doc_mask = doc_mask[doc_i:doc_i+1]\n",
    "                current_remaining_mask_np = np.ones_like(current_doc_mask ).astype(np.bool) | current_doc_mask\n",
    "                current_extraction_mask_np = np.zeros_like(current_doc_mask).astype(np.bool) | current_doc_mask\n",
    "        \n",
    "                current_global_context_embed = global_context_embed[doc_i:doc_i+1]\n",
    "                current_hidden_state = backward_state[ doc_i:doc_i+1 ]\n",
    "                current_extracted_sen_embed =  torch.zeros_like(  current_global_context_embed[:,:1,:] )\n",
    "        \n",
    "                current_hyps = []\n",
    "\n",
    "                for step in range( max_extracted_sentences_per_document ) :\n",
    "                    current_extraction_mask = torch.from_numpy( current_extraction_mask_np ).to(self.device)\n",
    "                    current_remaining_mask = torch.from_numpy( current_remaining_mask_np ).to(self.device)\n",
    "                    current_hidden_state  = self.extraction_context_decoder( current_extracted_sen_embed, current_hidden_state )\n",
    "                    p = self.extractor( current_global_context_embed, current_hidden_state, current_extraction_mask )\n",
    "                                                            \n",
    "                    sen_i = p.argmax(dim=1)[0]\n",
    "                    sen_i = sen_i.item()\n",
    "                    \n",
    "                    current_hyps.append(sen_i)\n",
    "                    current_extraction_mask_np[0, sen_i] = True\n",
    "                    current_remaining_mask_np[0, sen_i] = False\n",
    "                    \n",
    "                                        \n",
    "                extracted_sentences.append( [ document_batch[doc_i][sen_i] for sen_i in  current_hyps if sen_i < len(document_batch[doc_i])    ] )\n",
    "                extracted_sentences_positions.append( [ sen_i for sen_i in  current_hyps if sen_i < len(document_batch[doc_i])  ]  )\n",
    "\n",
    "\n",
    "        results = [extracted_sentences]\n",
    "        if return_sentence_position:\n",
    "            results.append( extracted_sentences_positions )\n",
    "        if len(results) == 1:\n",
    "            results = results[0]\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
